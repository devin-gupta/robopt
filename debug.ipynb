{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prettytable\n",
      "  Downloading prettytable-3.12.0-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.11/site-packages (from prettytable) (0.2.13)\n",
      "Downloading prettytable-3.12.0-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: prettytable\n",
      "Successfully installed prettytable-3.12.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as th\n",
    "from stable_baselines3 import PPO, SAC, TD3\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from envs.bidding import BiddingEnv  # Assuming this is your custom environment\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "# from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./runs/baselines\"\n",
    "\n",
    "\n",
    "env = BiddingEnv(render_mode='human')\n",
    "\n",
    "# Wrap with VecNormalize for observation and reward normalization\n",
    "vec_env = DummyVecEnv([lambda: env])  # Single process\n",
    "vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)\n",
    "\n",
    "\n",
    "# Load the pretrained model\n",
    "model = SAC.load(\"./runs/baselines/SAC/best_model\", env=vec_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "╒══════════╤══════════╤══════════╤══════════╤══════════╤══════════╕\n",
      "│   Task 1 │   Task 2 │   Task 3 │   Task 4 │   Task 5 │   Task 6 │\n",
      "╞══════════╪══════════╪══════════╪══════════╪══════════╪══════════╡\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "╘══════════╧══════════╧══════════╧══════════╧══════════╧══════════╛\n",
      "\n",
      "\n",
      "bidding matrix shape (9, 6)  and action shape  (9, 6)\n",
      "╒══════════╤══════════╤══════════╤══════════════╤══════════╤══════════════╕\n",
      "│   Task 1 │   Task 2 │   Task 3 │       Task 4 │   Task 5 │       Task 6 │\n",
      "╞══════════╪══════════╪══════════╪══════════════╪══════════╪══════════════╡\n",
      "│        0 │       10 │       10 │  0.118066    │        0 │  0           │\n",
      "├──────────┼──────────┼──────────┼──────────────┼──────────┼──────────────┤\n",
      "│        0 │       10 │        0 │  0.000850856 │       10 │  0           │\n",
      "├──────────┼──────────┼──────────┼──────────────┼──────────┼──────────────┤\n",
      "│       10 │       10 │       10 │ 10           │        0 │ 10           │\n",
      "├──────────┼──────────┼──────────┼──────────────┼──────────┼──────────────┤\n",
      "│       10 │        0 │       10 │  0           │        0 │  0           │\n",
      "├──────────┼──────────┼──────────┼──────────────┼──────────┼──────────────┤\n",
      "│       10 │        0 │       10 │  0           │        0 │ 10           │\n",
      "├──────────┼──────────┼──────────┼──────────────┼──────────┼──────────────┤\n",
      "│        0 │        0 │        0 │ 10           │       10 │ 10           │\n",
      "├──────────┼──────────┼──────────┼──────────────┼──────────┼──────────────┤\n",
      "│       10 │       10 │        0 │ 10           │       10 │ 10           │\n",
      "├──────────┼──────────┼──────────┼──────────────┼──────────┼──────────────┤\n",
      "│        0 │       10 │        0 │ 10           │       10 │  3.57628e-06 │\n",
      "├──────────┼──────────┼──────────┼──────────────┼──────────┼──────────────┤\n",
      "│       10 │        0 │       10 │  0           │       10 │  0           │\n",
      "╘══════════╧══════════╧══════════╧══════════════╧══════════╧══════════════╛\n",
      "\n",
      "\n",
      "Completed, final reward = 0.9639386917698692\n",
      "Optimal reward =  39.94526214587564\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Bidding space is invalid: [[0.00000000e+00 1.00000000e+01 1.00000000e+01 1.18066370e-01\n  0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+01 0.00000000e+00 8.50856304e-04\n  1.00000000e+01 0.00000000e+00]\n [1.00000000e+01 1.00000000e+01 1.00000000e+01 1.00000000e+01\n  0.00000000e+00 1.00000000e+01]\n [1.00000000e+01 0.00000000e+00 1.00000000e+01 0.00000000e+00\n  0.00000000e+00 0.00000000e+00]\n [1.00000000e+01 0.00000000e+00 1.00000000e+01 0.00000000e+00\n  0.00000000e+00 1.00000000e+01]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+01\n  1.00000000e+01 1.00000000e+01]\n [1.00000000e+01 1.00000000e+01 0.00000000e+00 1.00000000e+01\n  1.00000000e+01 1.00000000e+01]\n [0.00000000e+00 1.00000000e+01 0.00000000e+00 1.00000000e+01\n  1.00000000e+01 3.57627869e-06]\n [1.00000000e+01 0.00000000e+00 1.00000000e+01 0.00000000e+00\n  1.00000000e+01 0.00000000e+00]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted, final reward =\u001b[39m\u001b[38;5;124m\"\u001b[39m, reward)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptimal reward = \u001b[39m\u001b[38;5;124m'\u001b[39m, env\u001b[38;5;241m.\u001b[39moptimal_reward())\n\u001b[0;32m---> 17\u001b[0m obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Reward: \u001b[39m\u001b[38;5;124m\"\u001b[39m, reward)\n",
      "File \u001b[0;32m~/robopt/envs/bidding.py:128\u001b[0m, in \u001b[0;36mBiddingEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidding_matrix), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBidding space is invalid: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidding_matrix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# assert self.action_space.contains(action), f\"Invalid action: {action}\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# assert action.shape == (self.n_robots, self.n_tasks), f\"Invalid action shape: {action.shape}\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbidding matrix shape\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidding_matrix\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and action shape \u001b[39m\u001b[38;5;124m'\u001b[39m, action\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Bidding space is invalid: [[0.00000000e+00 1.00000000e+01 1.00000000e+01 1.18066370e-01\n  0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+01 0.00000000e+00 8.50856304e-04\n  1.00000000e+01 0.00000000e+00]\n [1.00000000e+01 1.00000000e+01 1.00000000e+01 1.00000000e+01\n  0.00000000e+00 1.00000000e+01]\n [1.00000000e+01 0.00000000e+00 1.00000000e+01 0.00000000e+00\n  0.00000000e+00 0.00000000e+00]\n [1.00000000e+01 0.00000000e+00 1.00000000e+01 0.00000000e+00\n  0.00000000e+00 1.00000000e+01]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+01\n  1.00000000e+01 1.00000000e+01]\n [1.00000000e+01 1.00000000e+01 0.00000000e+00 1.00000000e+01\n  1.00000000e+01 1.00000000e+01]\n [0.00000000e+00 1.00000000e+01 0.00000000e+00 1.00000000e+01\n  1.00000000e+01 3.57627869e-06]\n [1.00000000e+01 0.00000000e+00 1.00000000e+01 0.00000000e+00\n  1.00000000e+01 0.00000000e+00]]"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "from envs.bidding import BiddingEnv\n",
    "env = BiddingEnv()\n",
    "obs = env.reset()\n",
    "\n",
    "print(obs[1])\n",
    "\n",
    "env.render(mode='human')\n",
    "\n",
    "action, _ = model.predict(obs[0], deterministic=True) # use pretrained model\n",
    "# action = env.action_space.sample() # sample random action\n",
    "\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "env.render(mode='human')\n",
    "print(\"Completed, final reward =\", reward)\n",
    "print('Optimal reward = ', env.optimal_reward())\n",
    "obs, reward, done, truncated, info = env.step(env.action_space.sample())\n",
    "print(\"Random Reward: \", reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'robot_positions': array([[5, 8],\n",
      "       [5, 0],\n",
      "       [1, 7],\n",
      "       [6, 9],\n",
      "       [4, 5],\n",
      "       [4, 2],\n",
      "       [7, 7],\n",
      "       [1, 7],\n",
      "       [0, 6]], dtype=int32), 'robot_types': array([1, 0, 1, 2, 2, 0, 1, 2, 1], dtype=int32), 'task_positions': array([[9, 9],\n",
      "       [6, 9],\n",
      "       [1, 8],\n",
      "       [3, 9],\n",
      "       [7, 3],\n",
      "       [1, 9]], dtype=int32), 'task_prizes': array([ 8,  2, 89, 73, 71,  4], dtype=int32), 'task_types': array([1, 0, 1, 2, 1, 0], dtype=int32)}\n",
      "Using cpu device\n",
      "Logging to ./runs/baselines/SAC_2\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Invalid action: [[ 6.  0.  9.  5. 10.  7.]\n [ 4. 10.  9.  5.  5.  0.]\n [ 9.  6.  6.  6.  6.  1.]\n [ 8.  9.  0.  2.  3.  0.]\n [ 2.  9. 10.  8.  5.  8.]\n [ 8.  2.  1.  6.  9. 10.]\n [ 3.  2.  9.  1.  7.  0.]\n [ 8.  4.  2.  2.  0.  2.]\n [10.  9.  0.  8.  4.  9.]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m     17\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: env])\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m SAC(\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiInputPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Use MultiInputPolicy for Dict observation spaces\u001b[39;00m\n\u001b[1;32m     21\u001b[0m         vec_env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m     )\n\u001b[0;32m---> 27\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1e6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 1M steps\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(log_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAC_debug_model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# EVALUATE THE MODEL\u001b[39;00m\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/stable_baselines3/sac/sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[1;32m    300\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/stable_baselines3/common/off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/stable_baselines3/common/off_policy_algorithm.py:560\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    563\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/robopt/envs/bidding.py:128\u001b[0m, in \u001b[0;36mBiddingEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mcontains(action), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid action: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# assert action.shape == (self.n_robots, self.n_tasks), f\"Invalid action shape: {action.shape}\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbidding matrix shape\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidding_matrix\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and action shape \u001b[39m\u001b[38;5;124m'\u001b[39m, action\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Invalid action: [[ 6.  0.  9.  5. 10.  7.]\n [ 4. 10.  9.  5.  5.  0.]\n [ 9.  6.  6.  6.  6.  1.]\n [ 8.  9.  0.  2.  3.  0.]\n [ 2.  9. 10.  8.  5.  8.]\n [ 8.  2.  1.  6.  9. 10.]\n [ 3.  2.  9.  1.  7.  0.]\n [ 8.  4.  2.  2.  0.  2.]\n [10.  9.  0.  8.  4.  9.]]"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = BiddingEnv()\n",
    "\n",
    "obs, _ = env.reset()\n",
    "\n",
    "assert env.observation_space.contains(obs), f'obs not right {obs}'\n",
    "assert env.action_space.contains(env.bidding_matrix), f'action not right {env.bidding_matrix}'\n",
    "\n",
    "print(obs)\n",
    "\n",
    "# SETUP THE MODEL\n",
    "\n",
    "log_dir = \"./runs/baselines\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = SAC(\n",
    "        \"MultiInputPolicy\",  # Use MultiInputPolicy for Dict observation spaces\n",
    "        vec_env,\n",
    "        verbose=1,\n",
    "        tensorboard_log=log_dir,\n",
    "        device=\"auto\",\n",
    "    )\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=int(10),  # 1M steps\n",
    ")\n",
    "\n",
    "model.save(os.path.join(log_dir, f\"SAC_debug_model\"))\n",
    "\n",
    "# EVALUATE THE MODEL\n",
    "\n",
    "eval_env = DummyVecEnv([lambda: env])\n",
    "mean_reward, std_reward = model.evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "for i in range(100):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "    if env.action_space.contains(action):\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"Invalid action on step {i}:\", action)\n",
    "        raise ValueError(\"Invalid action\")\n",
    "\n",
    "print('completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
