{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Bidding (Baselines)\n",
    "\n",
    "In this walkthrough, we'll provide a brief example of how to use the custom bidding environment, as well as implementing some baselines. \n",
    "\n",
    "### Custom Environment Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Robot at (8, 5) with type navbot\n",
      "Robot at (6, 0) with type navbot\n",
      "Robot at (7, 1) with type humanbot\n",
      "Robot at (8, 3) with type navbot\n",
      "Robot at (2, 2) with type embedbot\n",
      "Robot at (9, 7) with type embedbot\n",
      "Robot at (3, 2) with type embedbot\n",
      "Robot at (9, 1) with type navbot\n",
      "Robot at (8, 4) with type humanbot\n",
      "\n",
      "Task at (4, 1) with prize 3 and type transport\n",
      "Task at (3, 7) with prize 2 and type specialty\n",
      "Task at (1, 4) with prize 2 and type transport\n",
      "Task at (9, 2) with prize 1 and type manipulation\n",
      "Task at (9, 5) with prize 3 and type transport\n",
      "Task at (9, 4) with prize 3 and type specialty\n",
      "\n",
      "Bidding Matrix: \n",
      "╒══════════╤══════════╤══════════╤══════════╤══════════╤══════════╕\n",
      "│   task 1 │   task 2 │   task 3 │   task 4 │   task 5 │   task 6 │\n",
      "╞══════════╪══════════╪══════════╪══════════╪══════════╪══════════╡\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        0 │        0 │        0 │        0 │        0 │        0 │\n",
      "╘══════════╧══════════╧══════════╧══════════╧══════════╧══════════╛\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import gymnasium as gym\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "# stable baselines\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# custom environment\n",
    "from envs.bidding import BiddingEnv\n",
    "\n",
    "env = BiddingEnv()\n",
    "\n",
    "env.render(mode='verbose') # choose from verbose, bids, plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: \n",
      " Dict('robot_0': Dict('bidding_matrix': Box(0, 10, (9, 6), int8), 'self_state': Box(0, [10 10  2], (3,), int8)), 'robot_1': Dict('bidding_matrix': Box(0, 10, (9, 6), int8), 'self_state': Box(0, [10 10  2], (3,), int8)), 'robot_2': Dict('bidding_matrix': Box(0, 10, (9, 6), int8), 'self_state': Box(0, [10 10  2], (3,), int8)), 'robot_3': Dict('bidding_matrix': Box(0, 10, (9, 6), int8), 'self_state': Box(0, [10 10  2], (3,), int8)), 'robot_4': Dict('bidding_matrix': Box(0, 10, (9, 6), int8), 'self_state': Box(0, [10 10  2], (3,), int8)), 'robot_5': Dict('bidding_matrix': Box(0, 10, (9, 6), int8), 'self_state': Box(0, [10 10  2], (3,), int8)), 'robot_6': Dict('bidding_matrix': Box(0, 10, (9, 6), int8), 'self_state': Box(0, [10 10  2], (3,), int8)), 'robot_7': Dict('bidding_matrix': Box(0, 10, (9, 6), int8), 'self_state': Box(0, [10 10  2], (3,), int8)), 'robot_8': Dict('bidding_matrix': Box(0, 10, (9, 6), int8), 'self_state': Box(0, [10 10  2], (3,), int8)))\n",
      "Action space: \n",
      " Box(0, 10, (9, 6), int8)\n",
      "Random Action (Bid Matrix): \n",
      " [[ 7  6  2  0  6  4]\n",
      " [ 6  1  5  7  7  5]\n",
      " [ 2  8  8  3  1  4]\n",
      " [ 6 10  7  2  8  7]\n",
      " [ 9  6 10  0  8  9]\n",
      " [ 6  5  8  3  4  4]\n",
      " [ 0  4  0  4  4  8]\n",
      " [ 9  1  1  7  7  5]\n",
      " [ 0  9  9  7  0  3]]\n",
      "\n",
      " Step 1 with reward = -0.10605038536102494\n",
      "\n",
      " Step 2 with reward = -0.46149656314919363\n",
      "\n",
      " Step 3 with reward = -0.13487326945615133\n",
      "\n",
      " Step 4 with reward = -0.19117534843828343\n",
      "\n",
      " Step 5 with reward = -0.2831064468352464\n",
      "\n",
      " Step 6 with reward = -0.1380422632941335\n",
      "\n",
      " Step 7 with reward = -0.19584190417023073\n",
      "\n",
      " Step 8 with reward = -0.3112629730163704\n",
      "\n",
      " Step 9 with reward = -0.3130917383486032\n",
      "\n",
      " Step 10 with reward = -42.18594544241716\n",
      "Step: 10\n",
      "Robot at (1, 1) with type embedbot\n",
      "Robot at (6, 1) with type humanbot\n",
      "Robot at (5, 6) with type navbot\n",
      "Robot at (9, 1) with type humanbot\n",
      "Robot at (1, 2) with type humanbot\n",
      "Robot at (3, 7) with type navbot\n",
      "Robot at (4, 1) with type navbot\n",
      "Robot at (3, 8) with type embedbot\n",
      "Robot at (4, 5) with type navbot\n",
      "\n",
      "Task at (0, 9) with prize 2 and type manipulation\n",
      "Task at (0, 7) with prize 2 and type transport\n",
      "Task at (8, 4) with prize 2 and type specialty\n",
      "Task at (3, 5) with prize 2 and type manipulation\n",
      "Task at (9, 9) with prize 1 and type transport\n",
      "Task at (7, 0) with prize 1 and type specialty\n",
      "\n",
      "Bidding Matrix: \n",
      "╒══════════╤══════════╤══════════╤══════════╤══════════╤══════════╕\n",
      "│   task 1 │   task 2 │   task 3 │   task 4 │   task 5 │   task 6 │\n",
      "╞══════════╪══════════╪══════════╪══════════╪══════════╪══════════╡\n",
      "│        5 │        2 │        4 │        5 │        0 │        3 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        8 │        6 │        6 │        2 │        4 │        6 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        9 │       10 │        3 │        0 │        8 │       10 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│       10 │        9 │        7 │        2 │        4 │        8 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        7 │       10 │        6 │        8 │        3 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        7 │        5 │        4 │        5 │        8 │        9 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        2 │        2 │        5 │        2 │        8 │        0 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        8 │        0 │        9 │        6 │        7 │        3 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│        3 │        3 │        9 │        8 │        0 │        4 │\n",
      "╘══════════╧══════════╧══════════╧══════════╧══════════╧══════════╛\n",
      "Completed, final reward = -42.18594544241716\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "print('Observation space: \\n', env.observation_space)\n",
    "print('Action space: \\n', env.action_space)\n",
    "print('Random Action (Bid Matrix): \\n', env.action_space.sample())\n",
    "\n",
    "for step in range(10):\n",
    "    \n",
    "    obs, reward, done, truncated, info = env.step(env.action_space.sample())\n",
    "    print(f'\\n Step {step + 1} with reward = {reward}')\n",
    "\n",
    "    if done:\n",
    "        env.render(mode='verbose')\n",
    "        print(\"Completed, final reward =\", reward)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking with Stable Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devg/robopt/.venv/lib/python3.11/site-packages/stable_baselines3/common/env_checker.py:93: UserWarning: Nested observation spaces are not supported by Stable Baselines3 (Dict spaces inside Dict space). You should flatten it to have only one level of keys.For example, `dict(space1=dict(space2=Box(), space3=Box()), spaces4=Discrete())` is not supported but `dict(space2=Box(), spaces3=Box(), spaces4=Discrete())` is.\n",
      "  warnings.warn(\n",
      "/Users/devg/robopt/.venv/lib/python3.11/site-packages/stable_baselines3/common/env_checker.py:453: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf. https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "  warnings.warn(\n",
      "/Users/devg/robopt/.venv/lib/python3.11/site-packages/stable_baselines3/common/env_checker.py:464: UserWarning: Your action space has dtype int8, we recommend using np.float32 to avoid cast errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env = BiddingEnv()\n",
    "\n",
    "# check stable baselines compatibility\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# tensorboard logging\n",
    "tensorboard_log_dir = \"./runs/bidding_stable_baselines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for random policy: -1546.5332292637909\n"
     ]
    }
   ],
   "source": [
    "# 1) baseline random policy\n",
    "class RandomPolicy:\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def predict(self, observation):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "# tensorboard random policy\n",
    "log_dir = \"./runs/baselines/random_policy\"\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# evaluate random policy\n",
    "random_policy = RandomPolicy(env.action_space)\n",
    "obs, _ = env.reset(seed=42)\n",
    "total_reward = 0\n",
    "\n",
    "for _ in range(1000):  # 1000 steps for example\n",
    "    action = random_policy.predict(obs)\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    # log to tensorboard\n",
    "    writer.add_scalar(\"Random_Policy/AvgBid\", np.mean(action), step)\n",
    "    writer.add_scalar(\"Random_Policy/StdBid\", np.std(action), step)\n",
    "    writer.add_scalar(\"Random_Policy/Reward\", reward, step)\n",
    "\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "print(f\"Total reward for random policy: {total_reward}\")\n",
    "\n",
    "writer.close() # close tensorboard writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Nested observation spaces are not supported (Tuple/Dict space inside Tuple/Dict space).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 2) baseline ppo policy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m \u001b[43mmake_vec_env\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mBiddingEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# vectorize env for ppo\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ppo_model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, vec_env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, tensorboard_log\u001b[38;5;241m=\u001b[39mtensorboard_log_dir)\n\u001b[1;32m      4\u001b[0m ppo_model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/stable_baselines3/common/env_util.py:125\u001b[0m, in \u001b[0;36mmake_vec_env\u001b[0;34m(env_id, n_envs, seed, start_index, monitor_dir, wrapper_class, env_kwargs, vec_env_cls, vec_env_kwargs, monitor_kwargs, wrapper_kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vec_env_cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# Default: use a DummyVecEnv\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     vec_env_cls \u001b[38;5;241m=\u001b[39m DummyVecEnv\n\u001b[0;32m--> 125\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m \u001b[43mvec_env_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmake_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_envs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvec_env_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Prepare the seeds for the first reset\u001b[39;00m\n\u001b[1;32m    127\u001b[0m vec_env\u001b[38;5;241m.\u001b[39mseed(seed)\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:44\u001b[0m, in \u001b[0;36mDummyVecEnv.__init__\u001b[0;34m(self, env_fns)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mlen\u001b[39m(env_fns), env\u001b[38;5;241m.\u001b[39mobservation_space, env\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[1;32m     43\u001b[0m obs_space \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys, shapes, dtypes \u001b[38;5;241m=\u001b[39m \u001b[43mobs_space_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_obs \u001b[38;5;241m=\u001b[39m OrderedDict([(k, np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mtuple\u001b[39m(shapes[k])), dtype\u001b[38;5;241m=\u001b[39mdtypes[k])) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys])\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/stable_baselines3/common/vec_env/util.py:61\u001b[0m, in \u001b[0;36mobs_space_info\u001b[0;34m(obs_space)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobs_space_info\u001b[39m(obs_space: spaces\u001b[38;5;241m.\u001b[39mSpace) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[Any, Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]], Dict[Any, np\u001b[38;5;241m.\u001b[39mdtype]]:\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Get dict-structured information about a gym.Space.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m        dtypes: a dict mapping keys to dtypes.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mcheck_for_nested_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs_space, spaces\u001b[38;5;241m.\u001b[39mDict):\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs_space\u001b[38;5;241m.\u001b[39mspaces, OrderedDict), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDict space must have ordered subspaces\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/stable_baselines3/common/preprocessing.py:225\u001b[0m, in \u001b[0;36mcheck_for_nested_spaces\u001b[0;34m(obs_space)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_space \u001b[38;5;129;01min\u001b[39;00m sub_spaces:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sub_space, (spaces\u001b[38;5;241m.\u001b[39mDict, spaces\u001b[38;5;241m.\u001b[39mTuple)):\n\u001b[0;32m--> 225\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNested observation spaces are not supported (Tuple/Dict space inside Tuple/Dict space).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    227\u001b[0m         )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Nested observation spaces are not supported (Tuple/Dict space inside Tuple/Dict space)."
     ]
    }
   ],
   "source": [
    "# 2) baseline ppo policy\n",
    "vec_env = make_vec_env(lambda: BiddingEnv(), n_envs=1) # vectorize env for ppo\n",
    "ppo_model = PPO(\"MlpPolicy\", vec_env, verbose=1, tensorboard_log=tensorboard_log_dir)\n",
    "ppo_model.learn(total_timesteps=10000)\n",
    "\n",
    "# evaluate ppo policy\n",
    "mean_reward, std_reward = evaluate_policy(ppo_model, vec_env, n_eval_episodes=10)\n",
    "print(f\"Mean reward for PPO: {mean_reward}, Std: {std_reward}\")\n",
    "\n",
    "# close envs (unnecessary in exisitng close() implementation)\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for heuristic policy: -1777.2110281092396\n"
     ]
    }
   ],
   "source": [
    "# 3) baseline heuristic policy\n",
    "class HeuristicPolicy:\n",
    "    def __init__(self, bidding_matrix):\n",
    "        self.bidding_matrix = bidding_matrix\n",
    "\n",
    "    def predict(self, observation):\n",
    "        # given prize range is (0, 4), always bid 2\n",
    "        return np.ones(self.bidding_matrix.shape) * 1\n",
    "\n",
    "# tensorboard random policy\n",
    "log_dir = \"./runs/baselines/heuristic_policy\"\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# evaluate heuristic policy\n",
    "heuristic_policy = HeuristicPolicy(env.bidding_matrix)\n",
    "obs, _ = env.reset(seed=42)\n",
    "total_reward = 0\n",
    "\n",
    "for _ in range(1000):  # 1000 steps for example\n",
    "    action = heuristic_policy.predict(obs)\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    # log to tensorboard\n",
    "    writer.add_scalar(\"Random_Policy/AvgBid\", np.mean(action), step)\n",
    "    writer.add_scalar(\"Random_Policy/StdBid\", np.std(action), step)\n",
    "    writer.add_scalar(\"Random_Policy/Reward\", reward, step)\n",
    "\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "print(f\"Total reward for heuristic policy: {total_reward}\")\n",
    "\n",
    "writer.close() # close tensorboard writer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
