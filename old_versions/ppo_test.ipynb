{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stable-baselines3\n",
      "  Downloading stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: gymnasium in ./.venv/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: tensorboard in ./.venv/lib/python3.11/site-packages (2.17.1)\n",
      "Collecting gymnasium\n",
      "  Using cached gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in ./.venv/lib/python3.11/site-packages (from stable-baselines3) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in ./.venv/lib/python3.11/site-packages (from stable-baselines3) (2.5.1)\n",
      "Requirement already satisfied: cloudpickle in ./.venv/lib/python3.11/site-packages (from stable-baselines3) (3.1.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from stable-baselines3) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.11/site-packages (from stable-baselines3) (3.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./.venv/lib/python3.11/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./.venv/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./.venv/lib/python3.11/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in ./.venv/lib/python3.11/site-packages (from tensorboard) (1.67.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.11/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from tensorboard) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in ./.venv/lib/python3.11/site-packages (from tensorboard) (4.25.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./.venv/lib/python3.11/site-packages (from tensorboard) (65.5.0)\n",
      "Requirement already satisfied: six>1.9 in ./.venv/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.11/site-packages (from tensorboard) (3.0.4)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3) (3.16.1)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib->stable-baselines3) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib->stable-baselines3) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib->stable-baselines3) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.11/site-packages (from matplotlib->stable-baselines3) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib->stable-baselines3) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->stable-baselines3) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->stable-baselines3) (2024.2)\n",
      "Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
      "Using cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "Installing collected packages: gymnasium, stable-baselines3\n",
      "  Attempting uninstall: gymnasium\n",
      "    Found existing installation: gymnasium 1.0.0\n",
      "    Uninstalling gymnasium-1.0.0:\n",
      "      Successfully uninstalled gymnasium-1.0.0\n",
      "Successfully installed gymnasium-0.29.1 stable-baselines3-2.3.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3 gymnasium tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./runs/bidding_ppo/PPO_2\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | -47.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 4442     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | -48.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2945         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002213335 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.3         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 983          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0118      |\n",
      "|    value_loss           | 2.97e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | -46.2         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2729          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00060468924 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -5.3          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 783           |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.014        |\n",
      "|    value_loss           | 1.68e+03      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -43.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2694        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024391416 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.28       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 587         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0913     |\n",
      "|    value_loss           | 1.24e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | -35.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2637       |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04393644 |\n",
      "|    clip_fraction        | 0.812      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.16      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 369        |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.154     |\n",
      "|    value_loss           | 878        |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from gymnasium.vector import SyncVectorEnv\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Custom callback to log actions to TensorBoard\n",
    "class ActionLoggingCallback(BaseCallback):\n",
    "    def __init__(self, log_dir, verbose=0):\n",
    "        super(ActionLoggingCallback, self).__init__(verbose)\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Log the actions taken by the model\n",
    "        action = self.locals[\"actions\"]  # Actions taken at the current step\n",
    "        episode = self.num_timesteps\n",
    "        self.writer.add_scalar(\"Action/Guessed_Bid\", action[0], episode)  # Log the first environment's action\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        self.writer.close()\n",
    "\n",
    "# Define the custom bidding environment\n",
    "class BiddingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(BiddingEnv, self).__init__()\n",
    "        self.action_space = gym.spaces.Discrete(200)  # Actions from 0 to 199\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)  # Optional seeding for reproducibility\n",
    "        self.state = np.array([0], dtype=np.float32)\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = -abs(action - 100)  # Max reward at bid = 100\n",
    "        self.state = np.array([0], dtype=np.float32)\n",
    "        done = True  # Single-step environment\n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "# Function to create an instance of the environment\n",
    "def make_env():\n",
    "    return BiddingEnv()\n",
    "\n",
    "# Instantiate and vectorize the environment\n",
    "env = SyncVectorEnv([make_env])\n",
    "\n",
    "# Set up the TensorBoard log directory\n",
    "tensorboard_log_dir = \"./runs/bidding_ppo\"\n",
    "callback = ActionLoggingCallback(log_dir=tensorboard_log_dir)\n",
    "\n",
    "# Initialize the PPO model with TensorBoard logging enabled\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=tensorboard_log_dir)\n",
    "\n",
    "# Start training, with custom logging of guessed values\n",
    "model.learn(total_timesteps=10000, callback=callback)\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "\n",
    "# Custom callback to log actions to TensorBoard\n",
    "class ActionLoggingCallback:\n",
    "    def __init__(self, log_dir):\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "        self.episode = 0\n",
    "\n",
    "    def log(self, action, reward):\n",
    "        # Log the action (guessed bid) and reward\n",
    "        self.writer.add_scalar(\"Action/Guessed_Bid\", action, self.episode)\n",
    "        self.writer.add_scalar(\"Reward/Episode_Reward\", reward, self.episode)\n",
    "        self.episode += 1\n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()\n",
    "\n",
    "# Define the custom bidding environment\n",
    "class BiddingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(BiddingEnv, self).__init__()\n",
    "        self.action_space = gym.spaces.Discrete(200)  # Actions from 0 to 199\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.state = np.array([0], dtype=np.float32)\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = -abs(action - 100)  # Max reward at bid = 100\n",
    "        self.state = np.array([0], dtype=np.float32)\n",
    "        done = True  # Single-step environment\n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "# Instantiate the environment\n",
    "env = BiddingEnv()\n",
    "\n",
    "# Initialize the Q-table\n",
    "q_table = np.zeros((1, env.action_space.n))  # Only one state in our environment\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "epsilon = 1.0  # Start with exploration\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n",
    "total_episodes = 10000\n",
    "\n",
    "# Set up TensorBoard logging\n",
    "tensorboard_log_dir = \"./runs/bidding_qlearning\"\n",
    "callback = ActionLoggingCallback(log_dir=tensorboard_log_dir)\n",
    "\n",
    "# Q-learning training loop\n",
    "for episode in range(total_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            action = np.argmax(q_table[0])  # Exploit known best action\n",
    "\n",
    "        # Take the chosen action and observe the reward\n",
    "        _, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Q-value update\n",
    "        old_q_value = q_table[0, action]\n",
    "        next_max = np.max(q_table[0])\n",
    "        q_table[0, action] = (1 - learning_rate) * old_q_value + learning_rate * (reward + discount_factor * next_max)\n",
    "\n",
    "        # Logging action and reward per episode\n",
    "        callback.log(action, total_reward)\n",
    "\n",
    "    # Decay epsilon to reduce exploration over time\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "callback.close()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Coded test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(200)  # Actions from 0 to 199\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
    "\n",
    "        def step(self, action):\n",
    "            # return observaton, reward, done, info\n",
    "            pass\n",
    "\n",
    "        def reset(self):\n",
    "            # return observaton\n",
    "            pass\n",
    "\n",
    "        def render(self, mode='human'):\n",
    "            pass\n",
    "\n",
    "        def close(self):\n",
    "            pass\n",
    "\n",
    "    \n",
    "env = CustomEnv()\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
