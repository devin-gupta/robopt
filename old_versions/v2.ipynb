{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "from other import *\n",
    "\n",
    "np.random.seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task:\n",
    "    def __init__(self, x, y):\n",
    "        self.name = \"Task\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.reward = np.random.randint(0, 100)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Task at ({self.x}, {self.y})\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Task at ({self.x}, {self.y}) with reward {self.reward}\"\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "    \n",
    "# Robot class with random position and speed initialization\n",
    "class Robot:\n",
    "    def __init__(self, grid_size):\n",
    "        # Random x, y position within grid boundaries\n",
    "        self.x = np.random.randint(0, grid_size[0])\n",
    "        self.y = np.random.randint(0, grid_size[1])\n",
    "        # Random speed between 0.5 and 1.0\n",
    "        self.speed = np.random.uniform(0.5, 1.0)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Robot at ({self.x}, {self.y}) with speed {self.speed:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58529937 7.07106781]\n"
     ]
    }
   ],
   "source": [
    "grid_size = (10, 10)\n",
    "num_robots = 3\n",
    "num_tasks = 5\n",
    "\n",
    "robots = [Robot(grid_size) for _ in range(num_robots)]\n",
    "\n",
    "tasks = [Task(\n",
    "    np.random.randint(0, grid_size[0]),  # random x position\n",
    "    np.random.randint(0, grid_size[1]))  # random y position\n",
    "    for _ in range(num_tasks)]\n",
    "\n",
    "rewards = [task.reward for task in tasks]\n",
    "\n",
    "# Initialize the r x t x 2 matrix\n",
    "matrix = np.zeros((num_robots, num_tasks, 2))\n",
    "\n",
    "for i, robot in enumerate(robots):\n",
    "    for j, task in enumerate(tasks):\n",
    "        # Calculate Euclidean distance\n",
    "        distance = np.sqrt((robot.x - task.x)**2 + (robot.y - task.y)**2)\n",
    "        \n",
    "        # Populate the matrix with (speed, distance)\n",
    "        matrix[i, j] = [robot.speed, distance]\n",
    "    \n",
    "print(matrix[1, 1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Epoch 1/100: Last guess = 1.88, Target = 1000, Avg Reward = -999.13\n",
      "Epoch 2/100: Last guess = 6.81, Target = 1000, Avg Reward = -995.79\n",
      "Epoch 3/100: Last guess = 18.28, Target = 1000, Avg Reward = -987.60\n",
      "Epoch 4/100: Last guess = 41.23, Target = 1000, Avg Reward = -970.29\n",
      "Epoch 5/100: Last guess = 82.13, Target = 1000, Avg Reward = -938.06\n",
      "Epoch 6/100: Last guess = 148.57, Target = 1000, Avg Reward = -883.76\n",
      "Epoch 7/100: Last guess = 249.41, Target = 1000, Avg Reward = -799.23\n",
      "Epoch 8/100: Last guess = 392.28, Target = 1000, Avg Reward = -675.63\n",
      "Epoch 9/100: Last guess = 576.97, Target = 1000, Avg Reward = -509.23\n",
      "Epoch 10/100: Last guess = 785.46, Target = 1000, Avg Reward = -308.82\n",
      "Epoch 11/100: Last guess = 967.52, Target = 1000, Avg Reward = -109.28\n",
      "Epoch 12/100: Last guess = 1057.57, Target = 1000, Avg Reward = -31.11\n",
      "Epoch 13/100: Last guess = 1052.30, Target = 1000, Avg Reward = -60.26\n",
      "Epoch 14/100: Last guess = 1014.82, Target = 1000, Avg Reward = -31.75\n",
      "Epoch 15/100: Last guess = 991.52, Target = 1000, Avg Reward = -5.55\n",
      "Epoch 16/100: Last guess = 989.01, Target = 1000, Avg Reward = -11.20\n",
      "Epoch 17/100: Last guess = 995.84, Target = 1000, Avg Reward = -7.46\n",
      "Epoch 18/100: Last guess = 1001.33, Target = 1000, Avg Reward = -1.40\n",
      "Epoch 19/100: Last guess = 1002.29, Target = 1000, Avg Reward = -2.18\n",
      "Epoch 20/100: Last guess = 1000.86, Target = 1000, Avg Reward = -1.57\n",
      "Epoch 21/100: Last guess = 999.67, Target = 1000, Avg Reward = -0.29\n",
      "Epoch 22/100: Last guess = 999.51, Target = 1000, Avg Reward = -0.49\n",
      "Epoch 23/100: Last guess = 999.86, Target = 1000, Avg Reward = -0.31\n",
      "Epoch 24/100: Last guess = 1000.10, Target = 1000, Avg Reward = -0.06\n",
      "Epoch 25/100: Last guess = 1000.10, Target = 1000, Avg Reward = -0.12\n",
      "Epoch 26/100: Last guess = 1000.01, Target = 1000, Avg Reward = -0.05\n",
      "Epoch 27/100: Last guess = 999.97, Target = 1000, Avg Reward = -0.02\n",
      "Epoch 28/100: Last guess = 999.98, Target = 1000, Avg Reward = -0.03\n",
      "Epoch 29/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.01\n",
      "Epoch 30/100: Last guess = 1000.01, Target = 1000, Avg Reward = -0.01\n",
      "Epoch 31/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 32/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 33/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 34/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 35/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 36/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 37/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 38/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 39/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 40/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 41/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 42/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 43/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 44/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 45/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 46/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 47/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 48/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 49/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 50/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 51/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 52/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 53/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 54/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 55/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 56/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 57/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 58/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 59/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 60/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 61/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 62/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 63/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 64/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 65/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 66/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 67/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 68/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 69/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 70/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 71/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 72/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 73/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 74/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 75/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 76/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 77/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 78/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 79/100: Last guess = 1000.00, Target = 1000, Avg Reward = -0.00\n",
      "Epoch 80/100: Last guess = 1000.00, Target = 1000, Avg Reward = 0.00\n",
      "Epoch 81/100: Last guess = 1000.00, Target = 1000, Avg Reward = 0.00\n",
      "Epoch 82/100: Last guess = 1000.00, Target = 1000, Avg Reward = 0.00\n",
      "Epoch 83/100: Last guess = 1000.00, Target = 1000, Avg Reward = 0.00\n",
      "Epoch 84/100: Last guess = 1000.00, Target = 1000, Avg Reward = 0.00\n",
      "Epoch 85/100: Last guess = 1000.00, Target = 1000, Avg Reward = 0.00\n",
      "Epoch 86/100: Last guess = 1000.00, Target = 1000, Avg Reward = 0.00\n",
      "Epoch 87/100: Last guess = 1000.00, Target = 1000, Avg Reward = 0.00\n",
      "Epoch 88/100: Last guess = 1000.00, Target = 1000, Avg Reward = 0.00\n",
      "Epoch 89/100: Last guess = 1000.00, Target = 1000, Avg Reward = 0.00\n",
      "Epoch 90/100: Last guess = 1000.00, Target = 1000, Avg Reward = 0.00\n",
      "Epoch 91/100: Last guess = 1000.00, Target = 1000, Avg Reward = 0.00\n",
      "Epoch 92/100: Last guess = 1000.00, Target = 1000, Avg Reward = 0.00\n",
      "Epoch 93/100: Last guess = 1000.00, Target = 1000, Avg Reward = 0.00\n",
      "Epoch 94/100: Last guess = 1000.00, Target = 1000, Avg Reward = 0.00\n",
      "Epoch 95/100: Last guess = 1000.00, Target = 1000, Avg Reward = 0.00\n",
      "Epoch 96/100: Last guess = 1000.00, Target = 1000, Avg Reward = 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m     target \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[target_val]], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Train the model to minimize the difference to the target\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Calculate the average reward for the epoch\u001b[39;00m\n\u001b[1;32m     65\u001b[0m avg_reward \u001b[38;5;241m=\u001b[39m epoch_reward \u001b[38;5;241m/\u001b[39m steps_per_epoch\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:282\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    275\u001b[0m     (\n\u001b[1;32m    276\u001b[0m         val_x,\n\u001b[1;32m    277\u001b[0m         val_y,\n\u001b[1;32m    278\u001b[0m         val_sample_weight,\n\u001b[1;32m    279\u001b[0m     ) \u001b[38;5;241m=\u001b[39m data_adapter_utils\u001b[38;5;241m.\u001b[39munpack_x_y_sample_weight(validation_data)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Create an iterator that yields batches for one epoch.\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m epoch_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_symbolic_build(iterator\u001b[38;5;241m=\u001b[39mepoch_iterator)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:666\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[0;32m--> 666\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedDataset):\n\u001b[1;32m    668\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(\n\u001b[1;32m    669\u001b[0m         dataset\n\u001b[1;32m    670\u001b[0m     )\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:675\u001b[0m, in \u001b[0;36mTFEpochIterator._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_iterator\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/array_data_adapter.py:232\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mwith_options(options)\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[0;32m--> 232\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mindices_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_batch_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    234\u001b[0m     indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mmap(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle)\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:2356\u001b[0m, in \u001b[0;36mDatasetV2.flat_map\u001b[0;34m(self, map_func, name)\u001b[0m\n\u001b[1;32m   2352\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> flat_map_op ->\u001b[39;00m\n\u001b[1;32m   2353\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2354\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2355\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flat_map_op\n\u001b[0;32m-> 2356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mflat_map_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/tensorflow/python/data/ops/flat_map_op.py:24\u001b[0m, in \u001b[0;36m_flat_map\u001b[0;34m(input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flat_map\u001b[39m(input_dataset, map_func, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_FlatMapDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/tensorflow/python/data/ops/flat_map_op.py:42\u001b[0m, in \u001b[0;36m_FlatMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39moutput_structure\u001b[38;5;241m.\u001b[39m_element_spec\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m---> 42\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_map_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_common_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[0;32m~/robopt/.venv/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:2426\u001b[0m, in \u001b[0;36mflat_map_dataset\u001b[0;34m(input_dataset, other_arguments, f, output_types, output_shapes, metadata, name)\u001b[0m\n\u001b[1;32m   2424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   2425\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2426\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2427\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFlatMapDataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_arguments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2428\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2429\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   2431\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import os\n",
    "\n",
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Define hyperparameters\n",
    "target_val = 1000  # Target value to be guessed\n",
    "num_epochs = 100\n",
    "steps_per_epoch = 10  # Number of guesses per epoch\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Define the input space dimension A\n",
    "A = 1\n",
    "\n",
    "# Initialize the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(A,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "    loss='mean_squared_error', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Create the log directory for TensorBoard\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "# Create a file writer for logging custom scalars\n",
    "file_writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_reward = 0  # Track the cumulative reward for each epoch\n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        # Generate a fixed input\n",
    "        input_val = np.array([[1]], dtype=np.float32)\n",
    "        \n",
    "        # Get the model's guess\n",
    "        guess = model(input_val, training=False)\n",
    "        \n",
    "        # Calculate reward as the negative of the absolute difference between guess and target_val\n",
    "        reward = -abs(guess - target_val)\n",
    "        \n",
    "        # Accumulate the reward for logging later\n",
    "        epoch_reward += reward\n",
    "        \n",
    "        # Create a \"target\" to minimize the loss\n",
    "        target = np.array([[target_val]], dtype=np.float32)\n",
    "        \n",
    "        # Train the model to minimize the difference to the target\n",
    "        model.fit(input_val, target, epochs=1, verbose=0, callbacks=[tensorboard_callback])\n",
    "    \n",
    "    # Calculate the average reward for the epoch\n",
    "    avg_reward = epoch_reward / steps_per_epoch\n",
    "    \n",
    "    # Log the reward to TensorBoard\n",
    "    with file_writer.as_default():\n",
    "        tf.summary.scalar('Average Reward', avg_reward[0][0], step=epoch)\n",
    "\n",
    "    # Print progress at the end of each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Last guess = {guess[0][0]:.2f}, Target = {target_val}, Avg Reward = {avg_reward[0][0]:.2f}\")\n",
    "\n",
    "# Close the file writer when training is complete\n",
    "file_writer.close()\n",
    "\n",
    "# Launch TensorBoard within the notebook\n",
    "%tensorboard --logdir logs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
