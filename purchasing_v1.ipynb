{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Learning to Guess Prize Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 0\n",
      "Reward: 0\n",
      "Task prize: 7\n",
      "Bidders' bids: [8, 10, 3, 6, 12, 8, 14, 14, 9, 0]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class Task:\n",
    "    def __init__(self):\n",
    "        self.prize = np.random.randint(5, 10)\n",
    "\n",
    "class Bidder:\n",
    "    def __init__(self):\n",
    "        self.bid = np.random.randint(0, 15)\n",
    "\n",
    "class PurchaseEnv(gym.Env):\n",
    "    def __init__(self, num_bidders=10):\n",
    "        super(PurchaseEnv, self).__init__()\n",
    "        self.task = Task()\n",
    "        self.bidders = [Bidder() for _ in range(num_bidders)]\n",
    "        self.action_space = spaces.Discrete(15)\n",
    "        self.observation_space = spaces.Discrete(15)\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.bidders = [Bidder() for _ in range(len(self.bidders))]\n",
    "        self.task = Task()\n",
    "        initial_observation = np.array([self.task.prize])\n",
    "        return initial_observation, {}\n",
    "    \n",
    "    def get_reward(self, action):\n",
    "        bids = [bidder.bid for bidder in self.bidders] + [action]\n",
    "        relevant_bids = [bid for bid in bids if bid <= self.task.prize]\n",
    "\n",
    "        if relevant_bids:\n",
    "            max_bid = max(relevant_bids)\n",
    "            if action == max_bid:\n",
    "                return self.task.prize\n",
    "            elif action > self.task.prize:\n",
    "                return -1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            if action > self.task.prize:\n",
    "                return -1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        terminated = True\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        reward = self.get_reward(action)\n",
    "        return np.array([self.task.prize]), reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Task prize: {self.task.prize}\")\n",
    "        print(f\"Bidders' bids: {[bidder.bid for bidder in self.bidders]}\")\n",
    "\n",
    "# Example usage\n",
    "env = PurchaseEnv()\n",
    "action = env.action_space.sample()\n",
    "print(\"Sampled action:\", action)\n",
    "\n",
    "obs = env.reset()\n",
    "# print(\"Initial observation:\", obs)\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "# print(\"Observation after step:\", obs)\n",
    "print(\"Reward:\", reward)\n",
    "# print(\"Done:\", done)\n",
    "# print(\"Info:\", info)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | -0.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 5544     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.04        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3950        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023008388 |\n",
      "|    clip_fraction        | 0.503       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.69       |\n",
      "|    explained_variance   | 0.000787    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.2         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0807     |\n",
      "|    value_loss           | 4.06        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 1.32       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3642       |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 1          |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04712695 |\n",
      "|    clip_fraction        | 0.521      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.61      |\n",
      "|    explained_variance   | 0.00902    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.18       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0902    |\n",
      "|    value_loss           | 4.94       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 2.56        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3481        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079454705 |\n",
      "|    clip_fraction        | 0.757       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.34       |\n",
      "|    explained_variance   | 0.0229      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.26        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.128      |\n",
      "|    value_loss           | 9.34        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 2.75       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3400       |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06303684 |\n",
      "|    clip_fraction        | 0.887      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.95      |\n",
      "|    explained_variance   | 0.0518     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.57       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.171     |\n",
      "|    value_loss           | 13.4       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 4.5        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3340       |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06743336 |\n",
      "|    clip_fraction        | 0.937      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | 0.0828     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.15       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.182     |\n",
      "|    value_loss           | 14         |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1         |\n",
      "|    ep_rew_mean          | 5.76      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 3267      |\n",
      "|    iterations           | 7         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 14336     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0698446 |\n",
      "|    clip_fraction        | 0.888     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.902    |\n",
      "|    explained_variance   | 0.123     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.81      |\n",
      "|    n_updates            | 60        |\n",
      "|    policy_gradient_loss | -0.167    |\n",
      "|    value_loss           | 11.8      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 6.09       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3248       |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12031664 |\n",
      "|    clip_fraction        | 0.374      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.407     |\n",
      "|    explained_variance   | 0.223      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.39       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.114     |\n",
      "|    value_loss           | 7.29       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 7.03       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3230       |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06388029 |\n",
      "|    clip_fraction        | 0.0518     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.124     |\n",
      "|    explained_variance   | 0.466      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.64       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0539    |\n",
      "|    value_loss           | 2.54       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 6.91        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3218        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009690305 |\n",
      "|    clip_fraction        | 0.0358      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.105      |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.354       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 0.618       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 6.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3216       |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03647566 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.101     |\n",
      "|    explained_variance   | 0.628      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.776      |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0455    |\n",
      "|    value_loss           | 1.5        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 7           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3213        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037855595 |\n",
      "|    clip_fraction        | 0.0155      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0314     |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.145       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0334     |\n",
      "|    value_loss           | 0.544       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 6.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3213        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008809148 |\n",
      "|    clip_fraction        | 0.0102      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.041      |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0293     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00411    |\n",
      "|    value_loss           | 0.103       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 6.74        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3208        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008288495 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.18       |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 6.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3207        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011090789 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.144      |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0767     |\n",
      "|    value_loss           | 1.83        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 6.91        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3199        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021180596 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0848     |\n",
      "|    explained_variance   | 0.653       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.58        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 6.96        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3174        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057506848 |\n",
      "|    clip_fraction        | 0.0225      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0239     |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.308       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    value_loss           | 0.806       |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 7.06          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3169          |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 11            |\n",
      "|    total_timesteps      | 36864         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00096063706 |\n",
      "|    clip_fraction        | 0.00254       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0124       |\n",
      "|    explained_variance   | 0.935         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.000696     |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.00384      |\n",
      "|    value_loss           | 0.115         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 6.47         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3170         |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024874145 |\n",
      "|    clip_fraction        | 0.00449      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0209      |\n",
      "|    explained_variance   | 0.97         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0352      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00305     |\n",
      "|    value_loss           | 0.0636       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 6.86        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3166        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038761705 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.104      |\n",
      "|    explained_variance   | -0.119      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.01        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0541     |\n",
      "|    value_loss           | 4.8         |\n",
      "-----------------------------------------\n",
      "Total reward after training with PPO: 8\n",
      "Task prize: 8\n",
      "Bidders' bids: [13, 1, 13, 4, 0, 10, 3, 9, 8, 12, 4, 10, 0, 1, 7, 8, 14, 5, 2, 3, 10, 10, 0, 5, 13, 14, 8, 12, 11, 1]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# Create the environment\n",
    "# env = DummyVecEnv([lambda: PurchaseEnv()])\n",
    "env = PurchaseEnv(num_bidders=30)\n",
    "\n",
    "# Instantiate the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, n_steps=2048, batch_size=64, n_epochs=10, learning_rate=3e-4)\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(4e4))\n",
    "\n",
    "# Test the trained agent\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total reward after training with PPO:\", total_reward)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Update the environment with 20 bidders\n",
    "env = PurchaseEnv(num_bidders=30)\n",
    "\n",
    "# Run random policy\n",
    "random_rewards = []\n",
    "for _ in range(1000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    random_rewards.append(total_reward)\n",
    "\n",
    "# Run trained policy\n",
    "ppo_rewards = []\n",
    "for _ in range(1000):\n",
    "    obs, _= env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    ppo_rewards.append(total_reward)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(random_rewards, bins=20, alpha=0.5, label='Random Policy', color='#ff7f0e', density=False)\n",
    "plt.hist(ppo_rewards, bins=20, alpha=0.5, label='Trained Policy', color='#1f77b4', density=False)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Agent Reward')\n",
    "plt.ylabel('Frequency (out of 1000)')\n",
    "plt.title('Definite Bidding Reward Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to dict observation spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 7\n",
      "Reward: -1\n",
      "Task prize: 6\n",
      "Bidders' bids: [10, 0, 0, 5, 5, 5, 5, 12, 6, 8]\n",
      "Bidders' distances: [2, 2, 1, 2, 3, 2, 3, 2, 1, 3]\n",
      "Distance: 2\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class PurchaseEnv(gym.Env):\n",
    "\n",
    "    class Task:\n",
    "        def __init__(self):\n",
    "            self.prize = np.random.randint(5, 10)\n",
    "\n",
    "    class Bidder:\n",
    "        def __init__(self):\n",
    "            self.bid = np.random.randint(0, 15)\n",
    "            self.distance = np.random.randint(1, 4)\n",
    "\n",
    "    def __init__(self, num_bidders=10):\n",
    "        super(PurchaseEnv, self).__init__()\n",
    "        self.task = self.Task()\n",
    "        self.bidders = [self.Bidder() for _ in range(num_bidders)]\n",
    "        self.distance = np.random.randint(1, 4)\n",
    "        self.action_space = spaces.Discrete(15)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'prize': spaces.Discrete(15),\n",
    "            'distance': spaces.Discrete(4)\n",
    "        })\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.bidders = [self.Bidder() for _ in range(len(self.bidders))]\n",
    "        self.task = self.Task()\n",
    "        self.distance = np.random.randint(1, 4)\n",
    "        initial_observation = {'prize': self.task.prize, 'distance': self.distance}\n",
    "        return initial_observation, {}\n",
    "    \n",
    "    def get_reward(self, action):\n",
    "        bids = [bidder.bid for bidder in self.bidders] + [action]\n",
    "        relevant_bids = [\n",
    "            bid for bid, bidder in zip(bids, self.bidders + [None])\n",
    "            if bid <= (self.task.prize - (bidder.distance if bidder else self.distance))\n",
    "        ]\n",
    "\n",
    "        if relevant_bids:\n",
    "            max_bid = max(relevant_bids)\n",
    "            if action == max_bid:\n",
    "                return self.task.prize - self.distance\n",
    "            elif action > self.task.prize:\n",
    "                return -1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return -1 if action > self.task.prize else 0\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        terminated = True\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        reward = self.get_reward(action)\n",
    "        return {'prize': self.task.prize, 'distance': self.distance}, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Task prize: {self.task.prize}\")\n",
    "        print(f\"Bidders' bids: {[bidder.bid for bidder in self.bidders]}\")\n",
    "        print(f\"Bidders' distances: {[bidder.distance for bidder in self.bidders]}\")\n",
    "        print(f\"Distance: {self.distance}\")\n",
    "\n",
    "# Example usage\n",
    "env = PurchaseEnv()\n",
    "action = env.action_space.sample()\n",
    "print(\"Sampled action:\", action)\n",
    "\n",
    "obs = env.reset()\n",
    "# print(\"Initial observation:\", obs)\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "# print(\"Observation after step:\", obs)\n",
    "print(\"Reward:\", reward)\n",
    "# print(\"Done:\", done)\n",
    "# print(\"Info:\", info)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./runs/baselines\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 4963     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3401        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026335169 |\n",
      "|    clip_fraction        | 0.573       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.69       |\n",
      "|    explained_variance   | -0.0258     |\n",
      "|    learning_rate        | 0.000294    |\n",
      "|    loss                 | 3.9         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0923     |\n",
      "|    value_loss           | 4.19        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=2.80 +/- 2.40\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 2.8         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027485877 |\n",
      "|    clip_fraction        | 0.657       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.0382      |\n",
      "|    learning_rate        | 0.000288    |\n",
      "|    loss                 | 2.46        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.105      |\n",
      "|    value_loss           | 4.92        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.38     |\n",
      "| time/              |          |\n",
      "|    fps             | 3132     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 1.19        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3111        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024194794 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.0403      |\n",
      "|    learning_rate        | 0.000282    |\n",
      "|    loss                 | 2.45        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0993     |\n",
      "|    value_loss           | 5.46        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=2.40 +/- 3.01\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 2.4         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025041314 |\n",
      "|    clip_fraction        | 0.616       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.35       |\n",
      "|    explained_variance   | 0.0622      |\n",
      "|    learning_rate        | 0.000275    |\n",
      "|    loss                 | 2.46        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.103      |\n",
      "|    value_loss           | 6.28        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 1.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 3094     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 1.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3089        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027856957 |\n",
      "|    clip_fraction        | 0.552       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.17       |\n",
      "|    explained_variance   | 0.0806      |\n",
      "|    learning_rate        | 0.000269    |\n",
      "|    loss                 | 3.17        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.094      |\n",
      "|    value_loss           | 6.59        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 1.89        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3081        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029832192 |\n",
      "|    clip_fraction        | 0.507       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.96       |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.000263    |\n",
      "|    loss                 | 3.24        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0931     |\n",
      "|    value_loss           | 7.04        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=3.00 +/- 2.76\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 3           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029328225 |\n",
      "|    clip_fraction        | 0.398       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 0.205       |\n",
      "|    learning_rate        | 0.000257    |\n",
      "|    loss                 | 3.13        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 6.7         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 2.39     |\n",
      "| time/              |          |\n",
      "|    fps             | 3070     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.01        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3069        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036594342 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.000251    |\n",
      "|    loss                 | 3.14        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 6.61        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=4.60 +/- 2.58\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 4.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023150053 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.000245    |\n",
      "|    loss                 | 2.97        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 5.8         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 2.72     |\n",
      "| time/              |          |\n",
      "|    fps             | 3060     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 2.99       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3058       |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02431989 |\n",
      "|    clip_fraction        | 0.203      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.18      |\n",
      "|    explained_variance   | 0.332      |\n",
      "|    learning_rate        | 0.000239   |\n",
      "|    loss                 | 2.56       |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0426    |\n",
      "|    value_loss           | 5.41       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.87        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3059        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022546005 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.373       |\n",
      "|    learning_rate        | 0.000232    |\n",
      "|    loss                 | 2.21        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    value_loss           | 5.19        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=4.40 +/- 2.33\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1         |\n",
      "|    mean_reward          | 4.4       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 25000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0210678 |\n",
      "|    clip_fraction        | 0.177     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.884    |\n",
      "|    explained_variance   | 0.42      |\n",
      "|    learning_rate        | 0.000226  |\n",
      "|    loss                 | 2.18      |\n",
      "|    n_updates            | 120       |\n",
      "|    policy_gradient_loss | -0.0386   |\n",
      "|    value_loss           | 4.58      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.48     |\n",
      "| time/              |          |\n",
      "|    fps             | 3056     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.28         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3057         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0104760695 |\n",
      "|    clip_fraction        | 0.132        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.763       |\n",
      "|    explained_variance   | 0.464        |\n",
      "|    learning_rate        | 0.00022      |\n",
      "|    loss                 | 1.91         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    value_loss           | 4.14         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=5.60 +/- 2.58\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 5.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009078873 |\n",
      "|    clip_fraction        | 0.0903      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.655      |\n",
      "|    explained_variance   | 0.488       |\n",
      "|    learning_rate        | 0.000214    |\n",
      "|    loss                 | 1.5         |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 4.03        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 3055     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.03        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3051        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007032875 |\n",
      "|    clip_fraction        | 0.0825      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.552      |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.000208    |\n",
      "|    loss                 | 2.87        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 3.86        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 3.96       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3046       |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00452823 |\n",
      "|    clip_fraction        | 0.0515     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.478     |\n",
      "|    explained_variance   | 0.508      |\n",
      "|    learning_rate        | 0.000202   |\n",
      "|    loss                 | 2.05       |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    value_loss           | 3.72       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=5.60 +/- 2.87\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 5.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003787373 |\n",
      "|    clip_fraction        | 0.0477      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.416      |\n",
      "|    explained_variance   | 0.559       |\n",
      "|    learning_rate        | 0.000196    |\n",
      "|    loss                 | 2.16        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 3.32        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 3023     |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.64         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3024         |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029926996 |\n",
      "|    clip_fraction        | 0.0474       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.366       |\n",
      "|    explained_variance   | 0.539        |\n",
      "|    learning_rate        | 0.000189     |\n",
      "|    loss                 | 1.24         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0106      |\n",
      "|    value_loss           | 3.25         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=3.40 +/- 2.87\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.4          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031369918 |\n",
      "|    clip_fraction        | 0.0378       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.319       |\n",
      "|    explained_variance   | 0.55         |\n",
      "|    learning_rate        | 0.000183     |\n",
      "|    loss                 | 2.22         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.009       |\n",
      "|    value_loss           | 3.3          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.9      |\n",
      "| time/              |          |\n",
      "|    fps             | 3025     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.85         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3024         |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018073367 |\n",
      "|    clip_fraction        | 0.0264       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.29        |\n",
      "|    explained_variance   | 0.574        |\n",
      "|    learning_rate        | 0.000177     |\n",
      "|    loss                 | 0.934        |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00828     |\n",
      "|    value_loss           | 3.19         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=2.60 +/- 2.33\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 2.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001689263 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.259      |\n",
      "|    explained_variance   | 0.571       |\n",
      "|    learning_rate        | 0.000171    |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00807    |\n",
      "|    value_loss           | 3.13        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.95     |\n",
      "| time/              |          |\n",
      "|    fps             | 3026     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.1          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3029         |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016478288 |\n",
      "|    clip_fraction        | 0.0242       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.225       |\n",
      "|    explained_variance   | 0.569        |\n",
      "|    learning_rate        | 0.000165     |\n",
      "|    loss                 | 1.27         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00626     |\n",
      "|    value_loss           | 3.1          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.27         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3032         |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013298166 |\n",
      "|    clip_fraction        | 0.0184       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.201       |\n",
      "|    explained_variance   | 0.571        |\n",
      "|    learning_rate        | 0.000159     |\n",
      "|    loss                 | 1.93         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00598     |\n",
      "|    value_loss           | 3.14         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=4.20 +/- 2.40\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 4.2          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011962494 |\n",
      "|    clip_fraction        | 0.0165       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.177       |\n",
      "|    explained_variance   | 0.556        |\n",
      "|    learning_rate        | 0.000153     |\n",
      "|    loss                 | 1.07         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00539     |\n",
      "|    value_loss           | 3.12         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 3033     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.51         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3033         |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008660297 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.157       |\n",
      "|    explained_variance   | 0.57         |\n",
      "|    learning_rate        | 0.000146     |\n",
      "|    loss                 | 1.13         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00512     |\n",
      "|    value_loss           | 3.11         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=6.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 6.2          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 55000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006363393 |\n",
      "|    clip_fraction        | 0.0107       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.143       |\n",
      "|    explained_variance   | 0.571        |\n",
      "|    learning_rate        | 0.00014      |\n",
      "|    loss                 | 1.62         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00275     |\n",
      "|    value_loss           | 3.1          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.72     |\n",
      "| time/              |          |\n",
      "|    fps             | 3032     |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3024        |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000593763 |\n",
      "|    clip_fraction        | 0.00732     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.135      |\n",
      "|    explained_variance   | 0.574       |\n",
      "|    learning_rate        | 0.000134    |\n",
      "|    loss                 | 1.78        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00257    |\n",
      "|    value_loss           | 3.07        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.45         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3024         |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005891856 |\n",
      "|    clip_fraction        | 0.00937      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.127       |\n",
      "|    explained_variance   | 0.571        |\n",
      "|    learning_rate        | 0.000128     |\n",
      "|    loss                 | 2            |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00416     |\n",
      "|    value_loss           | 3.1          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=4.20 +/- 2.48\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 4.2           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 60000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00070432376 |\n",
      "|    clip_fraction        | 0.00981       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.128        |\n",
      "|    explained_variance   | 0.602         |\n",
      "|    learning_rate        | 0.000122      |\n",
      "|    loss                 | 1.3           |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.0028       |\n",
      "|    value_loss           | 2.75          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.75     |\n",
      "| time/              |          |\n",
      "|    fps             | 3023     |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.04         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3024         |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008834238 |\n",
      "|    clip_fraction        | 0.0109       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.117       |\n",
      "|    explained_variance   | 0.517        |\n",
      "|    learning_rate        | 0.000116     |\n",
      "|    loss                 | 1.61         |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00389     |\n",
      "|    value_loss           | 3.4          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=3.20 +/- 2.64\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.2          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 65000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005032873 |\n",
      "|    clip_fraction        | 0.00757      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.109       |\n",
      "|    explained_variance   | 0.598        |\n",
      "|    learning_rate        | 0.00011      |\n",
      "|    loss                 | 1.72         |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00268     |\n",
      "|    value_loss           | 2.84         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 3025     |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.15         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3025         |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004562265 |\n",
      "|    clip_fraction        | 0.00708      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.103       |\n",
      "|    explained_variance   | 0.548        |\n",
      "|    learning_rate        | 0.000103     |\n",
      "|    loss                 | 1.21         |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00255     |\n",
      "|    value_loss           | 3.13         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.63         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3025         |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005107091 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0978      |\n",
      "|    explained_variance   | 0.573        |\n",
      "|    learning_rate        | 9.72e-05     |\n",
      "|    loss                 | 1.61         |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00333     |\n",
      "|    value_loss           | 3.04         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=3.20 +/- 3.06\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.2          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 70000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002894489 |\n",
      "|    clip_fraction        | 0.00654      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0925      |\n",
      "|    explained_variance   | 0.558        |\n",
      "|    learning_rate        | 9.11e-05     |\n",
      "|    loss                 | 1.08         |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    value_loss           | 3.23         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.3      |\n",
      "| time/              |          |\n",
      "|    fps             | 3025     |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.02         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3025         |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004686216 |\n",
      "|    clip_fraction        | 0.00762      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0853      |\n",
      "|    explained_variance   | 0.607        |\n",
      "|    learning_rate        | 8.5e-05      |\n",
      "|    loss                 | 0.77         |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.0025      |\n",
      "|    value_loss           | 2.76         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=6.00 +/- 0.63\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 6             |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 75000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031599027 |\n",
      "|    clip_fraction        | 0.00659       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0836       |\n",
      "|    explained_variance   | 0.586         |\n",
      "|    learning_rate        | 7.88e-05      |\n",
      "|    loss                 | 1.28          |\n",
      "|    n_updates            | 360           |\n",
      "|    policy_gradient_loss | -0.00202      |\n",
      "|    value_loss           | 2.94          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 3025     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.98         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3026         |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002866464 |\n",
      "|    clip_fraction        | 0.00356      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0816      |\n",
      "|    explained_variance   | 0.565        |\n",
      "|    learning_rate        | 7.27e-05     |\n",
      "|    loss                 | 1.5          |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00139     |\n",
      "|    value_loss           | 3.14         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.68          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3027          |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 79872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031875187 |\n",
      "|    clip_fraction        | 0.00352       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0786       |\n",
      "|    explained_variance   | 0.585         |\n",
      "|    learning_rate        | 6.65e-05      |\n",
      "|    loss                 | 1.46          |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | -0.00181      |\n",
      "|    value_loss           | 2.94          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=2.80 +/- 2.79\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 2.8           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 80000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029280598 |\n",
      "|    clip_fraction        | 0.00444       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0756       |\n",
      "|    explained_variance   | 0.573         |\n",
      "|    learning_rate        | 6.04e-05      |\n",
      "|    loss                 | 1.39          |\n",
      "|    n_updates            | 390           |\n",
      "|    policy_gradient_loss | -0.0012       |\n",
      "|    value_loss           | 3.03          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 3026     |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.47          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3019          |\n",
      "|    iterations           | 41            |\n",
      "|    time_elapsed         | 27            |\n",
      "|    total_timesteps      | 83968         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025786203 |\n",
      "|    clip_fraction        | 0.00386       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0739       |\n",
      "|    explained_variance   | 0.557         |\n",
      "|    learning_rate        | 5.42e-05      |\n",
      "|    loss                 | 1.42          |\n",
      "|    n_updates            | 400           |\n",
      "|    policy_gradient_loss | -0.00132      |\n",
      "|    value_loss           | 3.09          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=3.00 +/- 2.45\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 85000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002907592 |\n",
      "|    clip_fraction        | 0.00552      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0715      |\n",
      "|    explained_variance   | 0.588        |\n",
      "|    learning_rate        | 4.81e-05     |\n",
      "|    loss                 | 2.02         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    value_loss           | 2.93         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.1      |\n",
      "| time/              |          |\n",
      "|    fps             | 3014     |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.57         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3010         |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001347992 |\n",
      "|    clip_fraction        | 0.000928     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0711      |\n",
      "|    explained_variance   | 0.586        |\n",
      "|    learning_rate        | 4.2e-05      |\n",
      "|    loss                 | 0.877        |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00094     |\n",
      "|    value_loss           | 2.87         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=2.60 +/- 2.24\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 2.6           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 90000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015188588 |\n",
      "|    clip_fraction        | 0.00083       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0689       |\n",
      "|    explained_variance   | 0.557         |\n",
      "|    learning_rate        | 3.58e-05      |\n",
      "|    loss                 | 1.29          |\n",
      "|    n_updates            | 430           |\n",
      "|    policy_gradient_loss | -0.00131      |\n",
      "|    value_loss           | 3.07          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.85     |\n",
      "| time/              |          |\n",
      "|    fps             | 3006     |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.19         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3004         |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.888527e-05 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0678      |\n",
      "|    explained_variance   | 0.588        |\n",
      "|    learning_rate        | 2.97e-05     |\n",
      "|    loss                 | 1.36         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.000597    |\n",
      "|    value_loss           | 2.94         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.11          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3003          |\n",
      "|    iterations           | 46            |\n",
      "|    time_elapsed         | 31            |\n",
      "|    total_timesteps      | 94208         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.3405175e-05 |\n",
      "|    clip_fraction        | 0.00127       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0662       |\n",
      "|    explained_variance   | 0.561         |\n",
      "|    learning_rate        | 2.35e-05      |\n",
      "|    loss                 | 1.88          |\n",
      "|    n_updates            | 450           |\n",
      "|    policy_gradient_loss | -0.00075      |\n",
      "|    value_loss           | 3.18          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=3.80 +/- 2.32\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.8          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 95000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.551638e-05 |\n",
      "|    clip_fraction        | 0.000635     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0665      |\n",
      "|    explained_variance   | 0.584        |\n",
      "|    learning_rate        | 1.74e-05     |\n",
      "|    loss                 | 1.95         |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.000826    |\n",
      "|    value_loss           | 2.99         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 3001     |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 96256    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.9           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2996          |\n",
      "|    iterations           | 48            |\n",
      "|    time_elapsed         | 32            |\n",
      "|    total_timesteps      | 98304         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2488228e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0648       |\n",
      "|    explained_variance   | 0.561         |\n",
      "|    learning_rate        | 1.12e-05      |\n",
      "|    loss                 | 1.39          |\n",
      "|    n_updates            | 470           |\n",
      "|    policy_gradient_loss | -0.000333     |\n",
      "|    value_loss           | 3.1           |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=3.80 +/- 2.71\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 3.8           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 100000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1148775e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0652       |\n",
      "|    explained_variance   | 0.558         |\n",
      "|    learning_rate        | 5.09e-06      |\n",
      "|    loss                 | 2.09          |\n",
      "|    n_updates            | 480           |\n",
      "|    policy_gradient_loss | -0.000193     |\n",
      "|    value_loss           | 3.17          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 2993     |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "Total reward after training with PPO: 6\n",
      "Task prize: 7\n",
      "Bidders' bids: [10, 5, 7, 12, 13, 6, 4, 6, 10, 14]\n",
      "Bidders' distances: [1, 1, 2, 2, 3, 3, 2, 2, 3, 3]\n",
      "Distance: 1\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3.common.logger import configure\n",
    "from envs.purchasing import PurchaseEnv\n",
    "import os\n",
    "\n",
    "# Create the environment\n",
    "env = PurchaseEnv(num_bidders=10)\n",
    "\n",
    "# Configure TensorBoard logging\n",
    "log_dir = \"./runs/baselines\"\n",
    "new_logger = configure(log_dir, [\"stdout\", \"tensorboard\"])\n",
    "\n",
    "# Define a learning rate schedule\n",
    "def linear_schedule(initial_value):\n",
    "    def func(progress_remaining):\n",
    "        return progress_remaining * initial_value\n",
    "    return func\n",
    "\n",
    "# Instantiate the agent with adaptive learning rate\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=0, n_steps=2048, batch_size=64, n_epochs=10, learning_rate=linear_schedule(3e-4), tensorboard_log=log_dir)\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# Create callbacks for saving models and evaluation\n",
    "checkpoint_callback = CheckpointCallback(save_freq=int(1e4), save_path=f\"{log_dir}/checkpoints\", name_prefix='ppo_model')\n",
    "eval_callback = EvalCallback(env, best_model_save_path=f\"{log_dir}/best_model\", log_path=log_dir, eval_freq=5000, deterministic=True, render=False)\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(1e5), callback=callback)\n",
    "\n",
    "log_dir = \"./runs/baselines\"\n",
    "model_path = os.path.join(log_dir, \"best_model.zip\")\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path)\n",
    "\n",
    "# Test the trained agent\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total reward after training with PPO:\", total_reward)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Update the environment with 20 bidders\n",
    "env = PurchaseEnv(num_bidders=10)\n",
    "\n",
    "# Run random policy\n",
    "random_rewards = []\n",
    "for _ in range(1000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    random_rewards.append(total_reward)\n",
    "\n",
    "# Run trained policy\n",
    "ppo_rewards = []\n",
    "for _ in range(1000):\n",
    "    obs, _= env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    ppo_rewards.append(total_reward)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(random_rewards, bins=15, alpha=0.5, label='Random Policy', color='#ff7f0e', density=False)\n",
    "plt.hist(ppo_rewards, bins=15, alpha=0.5, label='PPO Trained Policy', color='#1f77b4', density=False)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Agent Reward')\n",
    "plt.ylabel('Frequency (out of 1000)')\n",
    "plt.title('Indefinite Bidding Reward Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task prize: 6\n",
      "Bidders' bids: [11, 10, 13, 8, 4, 6, 12, 10, 4, 12]\n",
      "Bidders' distances: [2, 3, 1, 2, 3, 1, 1, 1, 1, 1]\n",
      "Distance: 3\n",
      "\n",
      "With prize 6 and distance 3, chose action 3\n",
      "Got reward: 0\n"
     ]
    }
   ],
   "source": [
    "# Define a function to run the model and print the chosen action for different numbers of bidders\n",
    "env = PurchaseEnv(num_bidders=10)\n",
    "obs, _ = env.reset()\n",
    "action, _states = model.predict(obs)\n",
    "\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.render()\n",
    "\n",
    "print(f'\\nWith prize {obs[\"prize\"]} and distance {obs[\"distance\"]}, chose action {action}')\n",
    "\n",
    "print(f\"Got reward: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Task and Robot Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 8\n",
      "Reward: -1\n",
      "Task prize: 7\n",
      "Bidders' bids: [9, 0, 1, 0, 4, 14, 0, 14, 12, 13]\n",
      "Bidders' distances: [3, 1, 1, 2, 1, 2, 2, 2, 2, 3]\n",
      "Distance: 3\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class PurchaseEnv(gym.Env):\n",
    "\n",
    "    class Task:\n",
    "        def __init__(self):\n",
    "            self.prize = np.random.randint(5, 10)\n",
    "            self.type = np.random.randint(0, 2)\n",
    "\n",
    "    class Bidder:\n",
    "        def __init__(self):\n",
    "            self.bid = np.random.randint(0, 15)\n",
    "            self.distance = np.random.randint(1, 4)\n",
    "            self.type = np.random.randint(0, 2)\n",
    "\n",
    "    def __init__(self, num_bidders=10):\n",
    "        super(PurchaseEnv, self).__init__()\n",
    "        self.task = self.Task()\n",
    "        self.bidders = [self.Bidder() for _ in range(num_bidders)]\n",
    "        self.distance = np.random.randint(1, 4)\n",
    "        self.type = np.random.randint(0, 2)\n",
    "        self.action_space = spaces.Discrete(15)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'prize': spaces.Discrete(15),\n",
    "            'distance': spaces.Discrete(4),\n",
    "            'type': spaces.Discrete(2),\n",
    "        })\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.bidders = [self.Bidder() for _ in range(len(self.bidders))]\n",
    "        self.task = self.Task()\n",
    "        self.distance = np.random.randint(1, 4)\n",
    "        self.type = np.random.randint(0, 2)\n",
    "        initial_observation = {'prize': self.task.prize, 'distance': self.distance, 'type': self.type}\n",
    "        return initial_observation, {}\n",
    "    \n",
    "    def get_reward(self, action):\n",
    "\n",
    "        type_match = self.type == self.task.type\n",
    "\n",
    "        # filter out irrelevant bids\n",
    "        relevant_bids = []\n",
    "\n",
    "        for bid, bidder in zip([bidder.bid for bidder in self.bidders] + [action], self.bidders + [None]):\n",
    "            if type_match:\n",
    "                if bid <= (self.task.prize):\n",
    "                    relevant_bids.append(bid)\n",
    "            else:\n",
    "                if bid <= (self.task.prize - (bidder.distance if bidder else self.distance)):\n",
    "                    relevant_bids.append(bid)\n",
    "\n",
    "        assert relevant_bids, \"No relevant bids\"\n",
    "        \n",
    "        # determine reward\n",
    "        max_bid = max(relevant_bids)\n",
    "        if action == max_bid:\n",
    "            if type_match:\n",
    "                return self.task.prize\n",
    "            else:\n",
    "                return self.task.prize - self.distance\n",
    "        elif action > self.task.prize:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        terminated = True\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        reward = self.get_reward(action)\n",
    "        return {'prize': self.task.prize, 'distance': self.distance}, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Task prize: {self.task.prize}\")\n",
    "        print(f\"Bidders' bids: {[bidder.bid for bidder in self.bidders]}\")\n",
    "        print(f\"Bidders' distances: {[bidder.distance for bidder in self.bidders]}\")\n",
    "        print(f\"Distance: {self.distance}\")\n",
    "\n",
    "# Example usage\n",
    "env = PurchaseEnv()\n",
    "action = env.action_space.sample()\n",
    "print(\"Sampled action:\", action)\n",
    "\n",
    "obs = env.reset()\n",
    "# print(\"Initial observation:\", obs)\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "# print(\"Observation after step:\", obs)\n",
    "print(\"Reward:\", reward)\n",
    "# print(\"Done:\", done)\n",
    "# print(\"Info:\", info)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./runs/baselines\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.2      |\n",
      "| time/              |          |\n",
      "|    fps             | 5280     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.23        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3717        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026855508 |\n",
      "|    clip_fraction        | 0.587       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.69       |\n",
      "|    explained_variance   | -0.0174     |\n",
      "|    learning_rate        | 0.000294    |\n",
      "|    loss                 | 1.63        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0927     |\n",
      "|    value_loss           | 4.05        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=2.20 +/- 2.71\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | 2.2        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03212705 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.61      |\n",
      "|    explained_variance   | 0.036      |\n",
      "|    learning_rate        | 0.000288   |\n",
      "|    loss                 | 3.14       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.107     |\n",
      "|    value_loss           | 5.26       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devg/robopt/.venv/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 1.08     |\n",
      "| time/              |          |\n",
      "|    fps             | 3077     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.6         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3026        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027212795 |\n",
      "|    clip_fraction        | 0.664       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.0519      |\n",
      "|    learning_rate        | 0.000282    |\n",
      "|    loss                 | 2.98        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.106      |\n",
      "|    value_loss           | 6           |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=3.00 +/- 2.76\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 3           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027756479 |\n",
      "|    clip_fraction        | 0.603       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.33       |\n",
      "|    explained_variance   | 0.0783      |\n",
      "|    learning_rate        | 0.000275    |\n",
      "|    loss                 | 3.19        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.103      |\n",
      "|    value_loss           | 6.37        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 1.63     |\n",
      "| time/              |          |\n",
      "|    fps             | 2888     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 1.33       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2841       |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02574287 |\n",
      "|    clip_fraction        | 0.56       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.15      |\n",
      "|    explained_variance   | 0.115      |\n",
      "|    learning_rate        | 0.000269   |\n",
      "|    loss                 | 2.82       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0968    |\n",
      "|    value_loss           | 6.91       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 2           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2853        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032197554 |\n",
      "|    clip_fraction        | 0.493       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.94       |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.000263    |\n",
      "|    loss                 | 3.69        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0881     |\n",
      "|    value_loss           | 6.9         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=4.60 +/- 2.58\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 4.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030002713 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.000257    |\n",
      "|    loss                 | 3.04        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0768     |\n",
      "|    value_loss           | 6.63        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 2.26     |\n",
      "| time/              |          |\n",
      "|    fps             | 2855     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 2.56       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2861       |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03041262 |\n",
      "|    clip_fraction        | 0.333      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.52      |\n",
      "|    explained_variance   | 0.277      |\n",
      "|    learning_rate        | 0.000251   |\n",
      "|    loss                 | 2.75       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0632    |\n",
      "|    value_loss           | 6.32       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=3.80 +/- 3.37\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 3.8         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022982713 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.31        |\n",
      "|    learning_rate        | 0.000245    |\n",
      "|    loss                 | 3.21        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0586     |\n",
      "|    value_loss           | 5.86        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 2877     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.05        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2883        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026295947 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.342       |\n",
      "|    learning_rate        | 0.000239    |\n",
      "|    loss                 | 2.56        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 5.6         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2882        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019971728 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.396       |\n",
      "|    learning_rate        | 0.000232    |\n",
      "|    loss                 | 1.84        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    value_loss           | 4.99        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=5.00 +/- 1.79\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 5           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019090205 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.884      |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.000226    |\n",
      "|    loss                 | 1.86        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    value_loss           | 4.44        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.51     |\n",
      "| time/              |          |\n",
      "|    fps             | 2880     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2890        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010360936 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.745      |\n",
      "|    explained_variance   | 0.451       |\n",
      "|    learning_rate        | 0.00022     |\n",
      "|    loss                 | 1.3         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    value_loss           | 4.3         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=3.00 +/- 3.10\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1         |\n",
      "|    mean_reward          | 3         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 30000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0090287 |\n",
      "|    clip_fraction        | 0.113     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.633    |\n",
      "|    explained_variance   | 0.524     |\n",
      "|    learning_rate        | 0.000214  |\n",
      "|    loss                 | 1.49      |\n",
      "|    n_updates            | 140       |\n",
      "|    policy_gradient_loss | -0.021    |\n",
      "|    value_loss           | 3.75      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 2898     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.78         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2900         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069132345 |\n",
      "|    clip_fraction        | 0.0882       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.537       |\n",
      "|    explained_variance   | 0.503        |\n",
      "|    learning_rate        | 0.000208     |\n",
      "|    loss                 | 1.37         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0204      |\n",
      "|    value_loss           | 3.7          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2900        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005056572 |\n",
      "|    clip_fraction        | 0.0679      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.458      |\n",
      "|    explained_variance   | 0.495       |\n",
      "|    learning_rate        | 0.000202    |\n",
      "|    loss                 | 1.99        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 3.77        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=2.80 +/- 2.64\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 2.8          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 35000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036206897 |\n",
      "|    clip_fraction        | 0.0568       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.388       |\n",
      "|    explained_variance   | 0.531        |\n",
      "|    learning_rate        | 0.000196     |\n",
      "|    loss                 | 2.5          |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0132      |\n",
      "|    value_loss           | 3.53         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 2891     |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2898        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003963433 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.33       |\n",
      "|    explained_variance   | 0.544       |\n",
      "|    learning_rate        | 0.000189    |\n",
      "|    loss                 | 1.92        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 3.25        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=3.40 +/- 2.33\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.4          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013065682 |\n",
      "|    clip_fraction        | 0.023        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.291       |\n",
      "|    explained_variance   | 0.58         |\n",
      "|    learning_rate        | 0.000183     |\n",
      "|    loss                 | 1.51         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00707     |\n",
      "|    value_loss           | 3.05         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.48     |\n",
      "| time/              |          |\n",
      "|    fps             | 2886     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.75         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2887         |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021097977 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.259       |\n",
      "|    explained_variance   | 0.556        |\n",
      "|    learning_rate        | 0.000177     |\n",
      "|    loss                 | 1.7          |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00704     |\n",
      "|    value_loss           | 3.15         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=3.20 +/- 2.79\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 3.2         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001321423 |\n",
      "|    clip_fraction        | 0.0184      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.235      |\n",
      "|    explained_variance   | 0.565       |\n",
      "|    learning_rate        | 0.000171    |\n",
      "|    loss                 | 1.55        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00573    |\n",
      "|    value_loss           | 3.15        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.9      |\n",
      "| time/              |          |\n",
      "|    fps             | 2889     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.42         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2891         |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011776802 |\n",
      "|    clip_fraction        | 0.0139       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.212       |\n",
      "|    explained_variance   | 0.536        |\n",
      "|    learning_rate        | 0.000165     |\n",
      "|    loss                 | 1.46         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00511     |\n",
      "|    value_loss           | 3.3          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.11         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2896         |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008942466 |\n",
      "|    clip_fraction        | 0.0115       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.195       |\n",
      "|    explained_variance   | 0.568        |\n",
      "|    learning_rate        | 0.000159     |\n",
      "|    loss                 | 1.61         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00367     |\n",
      "|    value_loss           | 3.18         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=3.80 +/- 3.25\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.8          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010418843 |\n",
      "|    clip_fraction        | 0.0151       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.178       |\n",
      "|    explained_variance   | 0.581        |\n",
      "|    learning_rate        | 0.000153     |\n",
      "|    loss                 | 1.2          |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00427     |\n",
      "|    value_loss           | 3.1          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.38     |\n",
      "| time/              |          |\n",
      "|    fps             | 2896     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.64         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2886         |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010564842 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.162       |\n",
      "|    explained_variance   | 0.578        |\n",
      "|    learning_rate        | 0.000146     |\n",
      "|    loss                 | 2.11         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00512     |\n",
      "|    value_loss           | 2.96         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=4.80 +/- 1.17\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 4.8           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 55000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00087512063 |\n",
      "|    clip_fraction        | 0.0161        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.151        |\n",
      "|    explained_variance   | 0.575         |\n",
      "|    learning_rate        | 0.00014       |\n",
      "|    loss                 | 2.33          |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | -0.00507      |\n",
      "|    value_loss           | 3.1           |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.81     |\n",
      "| time/              |          |\n",
      "|    fps             | 2892     |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.92          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2896          |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 19            |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00066845247 |\n",
      "|    clip_fraction        | 0.00977       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.137        |\n",
      "|    explained_variance   | 0.603         |\n",
      "|    learning_rate        | 0.000134      |\n",
      "|    loss                 | 1.28          |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -0.0037       |\n",
      "|    value_loss           | 2.82          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.05          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2902          |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 20            |\n",
      "|    total_timesteps      | 59392         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00090757967 |\n",
      "|    clip_fraction        | 0.0149        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.125        |\n",
      "|    explained_variance   | 0.586         |\n",
      "|    learning_rate        | 0.000128      |\n",
      "|    loss                 | 2.2           |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -0.00448      |\n",
      "|    value_loss           | 2.91          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=3.20 +/- 2.93\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.2          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005181683 |\n",
      "|    clip_fraction        | 0.00713      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.115       |\n",
      "|    explained_variance   | 0.561        |\n",
      "|    learning_rate        | 0.000122     |\n",
      "|    loss                 | 1.19         |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    value_loss           | 3.14         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 2906     |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.17          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2911          |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 21            |\n",
      "|    total_timesteps      | 63488         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00065164047 |\n",
      "|    clip_fraction        | 0.0121        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.105        |\n",
      "|    explained_variance   | 0.554         |\n",
      "|    learning_rate        | 0.000116      |\n",
      "|    loss                 | 1.01          |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -0.00452      |\n",
      "|    value_loss           | 3.19          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=4.60 +/- 2.58\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 4.6          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 65000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004369716 |\n",
      "|    clip_fraction        | 0.0064       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0984      |\n",
      "|    explained_variance   | 0.556        |\n",
      "|    learning_rate        | 0.00011      |\n",
      "|    loss                 | 1.49         |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00182     |\n",
      "|    value_loss           | 3.2          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.86     |\n",
      "| time/              |          |\n",
      "|    fps             | 2915     |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.09          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2920          |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 23            |\n",
      "|    total_timesteps      | 67584         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039883587 |\n",
      "|    clip_fraction        | 0.00601       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0934       |\n",
      "|    explained_variance   | 0.579         |\n",
      "|    learning_rate        | 0.000103      |\n",
      "|    loss                 | 1.85          |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | -0.00203      |\n",
      "|    value_loss           | 3.05          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.38          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2925          |\n",
      "|    iterations           | 34            |\n",
      "|    time_elapsed         | 23            |\n",
      "|    total_timesteps      | 69632         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00022094222 |\n",
      "|    clip_fraction        | 0.004         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0893       |\n",
      "|    explained_variance   | 0.561         |\n",
      "|    learning_rate        | 9.72e-05      |\n",
      "|    loss                 | 1.35          |\n",
      "|    n_updates            | 330           |\n",
      "|    policy_gradient_loss | -0.00169      |\n",
      "|    value_loss           | 3.13          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=3.40 +/- 2.94\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.4          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 70000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004314135 |\n",
      "|    clip_fraction        | 0.0061       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0852      |\n",
      "|    explained_variance   | 0.555        |\n",
      "|    learning_rate        | 9.11e-05     |\n",
      "|    loss                 | 1.82         |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00208     |\n",
      "|    value_loss           | 3.11         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.09     |\n",
      "| time/              |          |\n",
      "|    fps             | 2927     |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.8           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2931          |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 25            |\n",
      "|    total_timesteps      | 73728         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00046830176 |\n",
      "|    clip_fraction        | 0.00869       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0798       |\n",
      "|    explained_variance   | 0.534         |\n",
      "|    learning_rate        | 8.5e-05       |\n",
      "|    loss                 | 2.14          |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | -0.0031       |\n",
      "|    value_loss           | 3.23          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=5.00 +/- 0.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 5           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 75000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000250242 |\n",
      "|    clip_fraction        | 0.00439     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0775     |\n",
      "|    explained_variance   | 0.598       |\n",
      "|    learning_rate        | 7.88e-05    |\n",
      "|    loss                 | 1.31        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00189    |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.95     |\n",
      "| time/              |          |\n",
      "|    fps             | 2934     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.81          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2936          |\n",
      "|    iterations           | 38            |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 77824         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033851515 |\n",
      "|    clip_fraction        | 0.00542       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0761       |\n",
      "|    explained_variance   | 0.57          |\n",
      "|    learning_rate        | 7.27e-05      |\n",
      "|    loss                 | 1.02          |\n",
      "|    n_updates            | 370           |\n",
      "|    policy_gradient_loss | -0.00276      |\n",
      "|    value_loss           | 2.92          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.14          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2937          |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 27            |\n",
      "|    total_timesteps      | 79872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032428018 |\n",
      "|    clip_fraction        | 0.00522       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0721       |\n",
      "|    explained_variance   | 0.582         |\n",
      "|    learning_rate        | 6.65e-05      |\n",
      "|    loss                 | 1.33          |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | -0.00181      |\n",
      "|    value_loss           | 2.9           |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=4.00 +/- 2.45\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 4             |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 80000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00042200307 |\n",
      "|    clip_fraction        | 0.00796       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0654       |\n",
      "|    explained_variance   | 0.571         |\n",
      "|    learning_rate        | 6.04e-05      |\n",
      "|    loss                 | 1.1           |\n",
      "|    n_updates            | 390           |\n",
      "|    policy_gradient_loss | -0.00375      |\n",
      "|    value_loss           | 3.07          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.98     |\n",
      "| time/              |          |\n",
      "|    fps             | 2940     |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.92          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2943          |\n",
      "|    iterations           | 41            |\n",
      "|    time_elapsed         | 28            |\n",
      "|    total_timesteps      | 83968         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015748892 |\n",
      "|    clip_fraction        | 0.00273       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.065        |\n",
      "|    explained_variance   | 0.578         |\n",
      "|    learning_rate        | 5.42e-05      |\n",
      "|    loss                 | 1.59          |\n",
      "|    n_updates            | 400           |\n",
      "|    policy_gradient_loss | -0.0013       |\n",
      "|    value_loss           | 2.96          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=4.40 +/- 2.42\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 4.4           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 85000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028698565 |\n",
      "|    clip_fraction        | 0.00469       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0649       |\n",
      "|    explained_variance   | 0.566         |\n",
      "|    learning_rate        | 4.81e-05      |\n",
      "|    loss                 | 1.69          |\n",
      "|    n_updates            | 410           |\n",
      "|    policy_gradient_loss | -0.00151      |\n",
      "|    value_loss           | 3.08          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.14     |\n",
      "| time/              |          |\n",
      "|    fps             | 2944     |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.83         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2946         |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001480561 |\n",
      "|    clip_fraction        | 0.00171      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0639      |\n",
      "|    explained_variance   | 0.569        |\n",
      "|    learning_rate        | 4.2e-05      |\n",
      "|    loss                 | 1.23         |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    value_loss           | 3.02         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=3.40 +/- 2.94\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 3.4           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 90000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021170778 |\n",
      "|    clip_fraction        | 0.00337       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0638       |\n",
      "|    explained_variance   | 0.571         |\n",
      "|    learning_rate        | 3.58e-05      |\n",
      "|    loss                 | 1.41          |\n",
      "|    n_updates            | 430           |\n",
      "|    policy_gradient_loss | -0.00153      |\n",
      "|    value_loss           | 3.07          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 2947     |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.29         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2941         |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001928889 |\n",
      "|    clip_fraction        | 0.00234      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0656      |\n",
      "|    explained_variance   | 0.535        |\n",
      "|    learning_rate        | 2.97e-05     |\n",
      "|    loss                 | 1.24         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.0018      |\n",
      "|    value_loss           | 3.36         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.15          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2939          |\n",
      "|    iterations           | 46            |\n",
      "|    time_elapsed         | 32            |\n",
      "|    total_timesteps      | 94208         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010148765 |\n",
      "|    clip_fraction        | 0.000488      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0638       |\n",
      "|    explained_variance   | 0.572         |\n",
      "|    learning_rate        | 2.35e-05      |\n",
      "|    loss                 | 1.96          |\n",
      "|    n_updates            | 450           |\n",
      "|    policy_gradient_loss | -0.000722     |\n",
      "|    value_loss           | 3.07          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=2.40 +/- 2.58\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 2.4           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 95000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014871074 |\n",
      "|    clip_fraction        | 0.0021        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0605       |\n",
      "|    explained_variance   | 0.576         |\n",
      "|    learning_rate        | 1.74e-05      |\n",
      "|    loss                 | 1.29          |\n",
      "|    n_updates            | 460           |\n",
      "|    policy_gradient_loss | -0.0016       |\n",
      "|    value_loss           | 2.98          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4        |\n",
      "| time/              |          |\n",
      "|    fps             | 2941     |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 96256    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.81         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2943         |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.578678e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0569      |\n",
      "|    explained_variance   | 0.588        |\n",
      "|    learning_rate        | 1.12e-05     |\n",
      "|    loss                 | 1.64         |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000447    |\n",
      "|    value_loss           | 2.85         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=5.00 +/- 2.83\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 5             |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 100000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7852395e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0567       |\n",
      "|    explained_variance   | 0.599         |\n",
      "|    learning_rate        | 5.09e-06      |\n",
      "|    loss                 | 1.47          |\n",
      "|    n_updates            | 480           |\n",
      "|    policy_gradient_loss | -0.000487     |\n",
      "|    value_loss           | 2.84          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.87     |\n",
      "| time/              |          |\n",
      "|    fps             | 2945     |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "Total reward after training with PPO: 4\n",
      "Task prize: 6\n",
      "Bidders' bids: [11, 7, 7, 14, 2, 14, 14, 9, 8, 14]\n",
      "Bidders' distances: [3, 1, 3, 3, 3, 1, 1, 2, 3, 2]\n",
      "Distance: 2\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3.common.logger import configure\n",
    "from envs.purchasing import PurchaseEnv\n",
    "import os\n",
    "\n",
    "# Create the environment\n",
    "env = PurchaseEnv(num_bidders=10)\n",
    "\n",
    "# Configure TensorBoard logging\n",
    "log_dir = \"./runs/baselines\"\n",
    "new_logger = configure(log_dir, [\"stdout\", \"tensorboard\"])\n",
    "\n",
    "# Define a learning rate schedule\n",
    "def linear_schedule(initial_value):\n",
    "    def func(progress_remaining):\n",
    "        return progress_remaining * initial_value\n",
    "    return func\n",
    "\n",
    "# Instantiate the agent with adaptive learning rate\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=0, n_steps=2048, batch_size=64, n_epochs=10, learning_rate=linear_schedule(3e-4), tensorboard_log=log_dir)\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# Create callbacks for saving models and evaluation\n",
    "checkpoint_callback = CheckpointCallback(save_freq=int(1e4), save_path=f\"{log_dir}/checkpoints\", name_prefix='ppo_model')\n",
    "eval_callback = EvalCallback(env, best_model_save_path=f\"{log_dir}/best_model\", log_path=log_dir, eval_freq=5000, deterministic=True, render=False)\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(1e5), callback=callback)\n",
    "\n",
    "log_dir = \"./runs/baselines\"\n",
    "model_path = os.path.join(log_dir, \"best_model.zip\")\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path)\n",
    "\n",
    "# Test the trained agent\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total reward after training with PPO:\", total_reward)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Update the environment with 20 bidders\n",
    "env = PurchaseEnv(num_bidders=20)\n",
    "\n",
    "# Run random policy\n",
    "random_rewards = []\n",
    "for _ in range(1000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    random_rewards.append(total_reward)\n",
    "\n",
    "# Run trained policy\n",
    "ppo_rewards = []\n",
    "for _ in range(1000):\n",
    "    obs, _= env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    ppo_rewards.append(total_reward)\n",
    "\n",
    "# Plot the results\n",
    "plt.hist(random_rewards, bins=20, alpha=0.5, label='Random Policy', color='blue', density=False)\n",
    "plt.hist(ppo_rewards, bins=20, alpha=0.5, label='Trained Policy', color='orange', density=False)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Total Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Total Rewards for Random and Trained Policies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
