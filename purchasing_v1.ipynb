{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Learning to Guess Prize Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 0\n",
      "Reward: 0\n",
      "Task prize: 6\n",
      "Bidders' bids: [9, 3, 14, 7, 8, 9, 2, 8, 14, 14]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class Task:\n",
    "    def __init__(self):\n",
    "        self.prize = np.random.randint(5, 10)\n",
    "\n",
    "class Bidder:\n",
    "    def __init__(self):\n",
    "        self.bid = np.random.randint(0, 15)\n",
    "\n",
    "class PurchaseEnv(gym.Env):\n",
    "    def __init__(self, num_bidders=10):\n",
    "        super(PurchaseEnv, self).__init__()\n",
    "        self.task = Task()\n",
    "        self.bidders = [Bidder() for _ in range(num_bidders)]\n",
    "        self.action_space = spaces.Discrete(15)\n",
    "        self.observation_space = spaces.Discrete(15)\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.bidders = [Bidder() for _ in range(len(self.bidders))]\n",
    "        self.task = Task()\n",
    "        initial_observation = np.array([self.task.prize])\n",
    "        return initial_observation, {}\n",
    "    \n",
    "    def get_reward(self, action):\n",
    "        bids = [bidder.bid for bidder in self.bidders] + [action]\n",
    "        relevant_bids = [bid for bid in bids if bid <= self.task.prize]\n",
    "\n",
    "        if relevant_bids:\n",
    "            max_bid = max(relevant_bids)\n",
    "            if action == max_bid:\n",
    "                return self.task.prize\n",
    "            elif action > self.task.prize:\n",
    "                return -1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            if action > self.task.prize:\n",
    "                return -1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        terminated = True\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        reward = self.get_reward(action)\n",
    "        return np.array([self.task.prize]), reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Task prize: {self.task.prize}\")\n",
    "        print(f\"Bidders' bids: {[bidder.bid for bidder in self.bidders]}\")\n",
    "\n",
    "# Example usage\n",
    "env = PurchaseEnv()\n",
    "action = env.action_space.sample()\n",
    "print(\"Sampled action:\", action)\n",
    "\n",
    "obs = env.reset()\n",
    "# print(\"Initial observation:\", obs)\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "# print(\"Observation after step:\", obs)\n",
    "print(\"Reward:\", reward)\n",
    "# print(\"Done:\", done)\n",
    "# print(\"Info:\", info)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.17     |\n",
      "| time/              |          |\n",
      "|    fps             | 4531     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.24       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3616       |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 1          |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02562379 |\n",
      "|    clip_fraction        | 0.481      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.69      |\n",
      "|    explained_variance   | -0.0067    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.06       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0823    |\n",
      "|    value_loss           | 4.25       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3424        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039998464 |\n",
      "|    clip_fraction        | 0.527       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.00431     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.44        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.09       |\n",
      "|    value_loss           | 4.78        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 3.08       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3358       |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 2          |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08219638 |\n",
      "|    clip_fraction        | 0.752      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.35      |\n",
      "|    explained_variance   | 0.0131     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.35       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.124     |\n",
      "|    value_loss           | 8.45       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.03        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3309        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060674377 |\n",
      "|    clip_fraction        | 0.874       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.96       |\n",
      "|    explained_variance   | 0.0441      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.14        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.166      |\n",
      "|    value_loss           | 13          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 4.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3260        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054256476 |\n",
      "|    clip_fraction        | 0.928       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.0588      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.26        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.179      |\n",
      "|    value_loss           | 13.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 5.9         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3225        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068968646 |\n",
      "|    clip_fraction        | 0.913       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.957      |\n",
      "|    explained_variance   | 0.0718      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.94        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.17       |\n",
      "|    value_loss           | 12.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 6.62       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3217       |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16509634 |\n",
      "|    clip_fraction        | 0.378      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.4       |\n",
      "|    explained_variance   | 0.156      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.44       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.124     |\n",
      "|    value_loss           | 8.19       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 6.96        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3204        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039848097 |\n",
      "|    clip_fraction        | 0.0365      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.136      |\n",
      "|    explained_variance   | 0.511       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    value_loss           | 1.91        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 6.87        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3202        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001645278 |\n",
      "|    clip_fraction        | 0.0106      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0737     |\n",
      "|    explained_variance   | 0.779       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.25        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 0.528       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 6.93         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3198         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068591516 |\n",
      "|    clip_fraction        | 0.0129       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0427      |\n",
      "|    explained_variance   | 0.742        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.866        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.0175      |\n",
      "|    value_loss           | 0.701        |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 6.39       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3196       |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06811294 |\n",
      "|    clip_fraction        | 0.0762     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.114     |\n",
      "|    explained_variance   | 0.919      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0551    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    value_loss           | 0.175      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 6.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3183        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016022991 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.207      |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.57        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 6.91         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3173         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074928706 |\n",
      "|    clip_fraction        | 0.178        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.179       |\n",
      "|    explained_variance   | 0.507        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.48         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0722      |\n",
      "|    value_loss           | 2.49         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 6.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3166        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011574925 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.132      |\n",
      "|    explained_variance   | 0.541       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.627       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0741     |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 7.1        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3165       |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03900774 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0517    |\n",
      "|    explained_variance   | 0.556      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.569      |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0698    |\n",
      "|    value_loss           | 1.45       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 7.24        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3164        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011690569 |\n",
      "|    clip_fraction        | 0.00708     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0241     |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0115     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 0.358       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 7.07         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3147         |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003507245 |\n",
      "|    clip_fraction        | 0.00259      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0197      |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000497    |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00379     |\n",
      "|    value_loss           | 0.158        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 6.96          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3144          |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 12            |\n",
      "|    total_timesteps      | 38912         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013795198 |\n",
      "|    clip_fraction        | 0.00161       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.018        |\n",
      "|    explained_variance   | 0.959         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00181      |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -0.00292      |\n",
      "|    value_loss           | 0.0852        |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 6.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3141        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004519419 |\n",
      "|    clip_fraction        | 0.00273     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0141     |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00318    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00672    |\n",
      "|    value_loss           | 0.0946      |\n",
      "-----------------------------------------\n",
      "Total reward after training with PPO: 8\n",
      "Task prize: 8\n",
      "Bidders' bids: [5, 11, 12, 8, 5, 13, 3, 14, 12, 2, 2, 5, 4, 9, 12, 5, 12, 8, 3, 5, 3, 10, 8, 2, 5, 5, 9, 9, 9, 5]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# Create the environment\n",
    "# env = DummyVecEnv([lambda: PurchaseEnv()])\n",
    "env = PurchaseEnv(num_bidders=30)\n",
    "\n",
    "# Instantiate the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, n_steps=2048, batch_size=64, n_epochs=10, learning_rate=3e-4)\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(4e4))\n",
    "\n",
    "# Test the trained agent\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total reward after training with PPO:\", total_reward)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Update the environment with 20 bidders\n",
    "env = PurchaseEnv(num_bidders=30)\n",
    "\n",
    "# Run random policy\n",
    "random_rewards = []\n",
    "for _ in range(100):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    random_rewards.append(total_reward)\n",
    "\n",
    "# Run trained policy\n",
    "ppo_rewards = []\n",
    "for _ in range(100):\n",
    "    obs, _= env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    ppo_rewards.append(total_reward)\n",
    "\n",
    "# Plot the results\n",
    "plt.hist(random_rewards, bins=20, alpha=0.5, label='Random Policy', color='blue', density=True)\n",
    "plt.hist(ppo_rewards, bins=20, alpha=0.5, label='Trained Policy', color='orange', density=True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Total Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Total Rewards for Random and Trained Policies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to dict observation spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 7\n",
      "Reward: -1\n",
      "Task prize: 6\n",
      "Bidders' bids: [10, 0, 0, 5, 5, 5, 5, 12, 6, 8]\n",
      "Bidders' distances: [2, 2, 1, 2, 3, 2, 3, 2, 1, 3]\n",
      "Distance: 2\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class PurchaseEnv(gym.Env):\n",
    "\n",
    "    class Task:\n",
    "        def __init__(self):\n",
    "            self.prize = np.random.randint(5, 10)\n",
    "\n",
    "    class Bidder:\n",
    "        def __init__(self):\n",
    "            self.bid = np.random.randint(0, 15)\n",
    "            self.distance = np.random.randint(1, 4)\n",
    "\n",
    "    def __init__(self, num_bidders=10):\n",
    "        super(PurchaseEnv, self).__init__()\n",
    "        self.task = self.Task()\n",
    "        self.bidders = [self.Bidder() for _ in range(num_bidders)]\n",
    "        self.distance = np.random.randint(1, 4)\n",
    "        self.action_space = spaces.Discrete(15)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'prize': spaces.Discrete(15),\n",
    "            'distance': spaces.Discrete(4)\n",
    "        })\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.bidders = [self.Bidder() for _ in range(len(self.bidders))]\n",
    "        self.task = self.Task()\n",
    "        self.distance = np.random.randint(1, 4)\n",
    "        initial_observation = {'prize': self.task.prize, 'distance': self.distance}\n",
    "        return initial_observation, {}\n",
    "    \n",
    "    def get_reward(self, action):\n",
    "        bids = [bidder.bid for bidder in self.bidders] + [action]\n",
    "        relevant_bids = [\n",
    "            bid for bid, bidder in zip(bids, self.bidders + [None])\n",
    "            if bid <= (self.task.prize - (bidder.distance if bidder else self.distance))\n",
    "        ]\n",
    "\n",
    "        if relevant_bids:\n",
    "            max_bid = max(relevant_bids)\n",
    "            if action == max_bid:\n",
    "                return self.task.prize - self.distance\n",
    "            elif action > self.task.prize:\n",
    "                return -1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return -1 if action > self.task.prize else 0\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        terminated = True\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        reward = self.get_reward(action)\n",
    "        return {'prize': self.task.prize, 'distance': self.distance}, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Task prize: {self.task.prize}\")\n",
    "        print(f\"Bidders' bids: {[bidder.bid for bidder in self.bidders]}\")\n",
    "        print(f\"Bidders' distances: {[bidder.distance for bidder in self.bidders]}\")\n",
    "        print(f\"Distance: {self.distance}\")\n",
    "\n",
    "# Example usage\n",
    "env = PurchaseEnv()\n",
    "action = env.action_space.sample()\n",
    "print(\"Sampled action:\", action)\n",
    "\n",
    "obs = env.reset()\n",
    "# print(\"Initial observation:\", obs)\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "# print(\"Observation after step:\", obs)\n",
    "print(\"Reward:\", reward)\n",
    "# print(\"Done:\", done)\n",
    "# print(\"Info:\", info)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./runs/baselines\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 4963     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3401        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026335169 |\n",
      "|    clip_fraction        | 0.573       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.69       |\n",
      "|    explained_variance   | -0.0258     |\n",
      "|    learning_rate        | 0.000294    |\n",
      "|    loss                 | 3.9         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0923     |\n",
      "|    value_loss           | 4.19        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=2.80 +/- 2.40\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 2.8         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027485877 |\n",
      "|    clip_fraction        | 0.657       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.0382      |\n",
      "|    learning_rate        | 0.000288    |\n",
      "|    loss                 | 2.46        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.105      |\n",
      "|    value_loss           | 4.92        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.38     |\n",
      "| time/              |          |\n",
      "|    fps             | 3132     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 1.19        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3111        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024194794 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.0403      |\n",
      "|    learning_rate        | 0.000282    |\n",
      "|    loss                 | 2.45        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0993     |\n",
      "|    value_loss           | 5.46        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=2.40 +/- 3.01\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 2.4         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025041314 |\n",
      "|    clip_fraction        | 0.616       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.35       |\n",
      "|    explained_variance   | 0.0622      |\n",
      "|    learning_rate        | 0.000275    |\n",
      "|    loss                 | 2.46        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.103      |\n",
      "|    value_loss           | 6.28        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 1.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 3094     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 1.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3089        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027856957 |\n",
      "|    clip_fraction        | 0.552       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.17       |\n",
      "|    explained_variance   | 0.0806      |\n",
      "|    learning_rate        | 0.000269    |\n",
      "|    loss                 | 3.17        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.094      |\n",
      "|    value_loss           | 6.59        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 1.89        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3081        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029832192 |\n",
      "|    clip_fraction        | 0.507       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.96       |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.000263    |\n",
      "|    loss                 | 3.24        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0931     |\n",
      "|    value_loss           | 7.04        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=3.00 +/- 2.76\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 3           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029328225 |\n",
      "|    clip_fraction        | 0.398       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 0.205       |\n",
      "|    learning_rate        | 0.000257    |\n",
      "|    loss                 | 3.13        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 6.7         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 2.39     |\n",
      "| time/              |          |\n",
      "|    fps             | 3070     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.01        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3069        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036594342 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.000251    |\n",
      "|    loss                 | 3.14        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 6.61        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=4.60 +/- 2.58\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 4.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023150053 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.000245    |\n",
      "|    loss                 | 2.97        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 5.8         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 2.72     |\n",
      "| time/              |          |\n",
      "|    fps             | 3060     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 2.99       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3058       |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02431989 |\n",
      "|    clip_fraction        | 0.203      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.18      |\n",
      "|    explained_variance   | 0.332      |\n",
      "|    learning_rate        | 0.000239   |\n",
      "|    loss                 | 2.56       |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0426    |\n",
      "|    value_loss           | 5.41       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.87        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3059        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022546005 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.373       |\n",
      "|    learning_rate        | 0.000232    |\n",
      "|    loss                 | 2.21        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    value_loss           | 5.19        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=4.40 +/- 2.33\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1         |\n",
      "|    mean_reward          | 4.4       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 25000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0210678 |\n",
      "|    clip_fraction        | 0.177     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.884    |\n",
      "|    explained_variance   | 0.42      |\n",
      "|    learning_rate        | 0.000226  |\n",
      "|    loss                 | 2.18      |\n",
      "|    n_updates            | 120       |\n",
      "|    policy_gradient_loss | -0.0386   |\n",
      "|    value_loss           | 4.58      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.48     |\n",
      "| time/              |          |\n",
      "|    fps             | 3056     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.28         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3057         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0104760695 |\n",
      "|    clip_fraction        | 0.132        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.763       |\n",
      "|    explained_variance   | 0.464        |\n",
      "|    learning_rate        | 0.00022      |\n",
      "|    loss                 | 1.91         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    value_loss           | 4.14         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=5.60 +/- 2.58\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 5.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009078873 |\n",
      "|    clip_fraction        | 0.0903      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.655      |\n",
      "|    explained_variance   | 0.488       |\n",
      "|    learning_rate        | 0.000214    |\n",
      "|    loss                 | 1.5         |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 4.03        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 3055     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.03        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3051        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007032875 |\n",
      "|    clip_fraction        | 0.0825      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.552      |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.000208    |\n",
      "|    loss                 | 2.87        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 3.86        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 3.96       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3046       |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00452823 |\n",
      "|    clip_fraction        | 0.0515     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.478     |\n",
      "|    explained_variance   | 0.508      |\n",
      "|    learning_rate        | 0.000202   |\n",
      "|    loss                 | 2.05       |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    value_loss           | 3.72       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=5.60 +/- 2.87\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 5.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003787373 |\n",
      "|    clip_fraction        | 0.0477      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.416      |\n",
      "|    explained_variance   | 0.559       |\n",
      "|    learning_rate        | 0.000196    |\n",
      "|    loss                 | 2.16        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 3.32        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 3023     |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.64         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3024         |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029926996 |\n",
      "|    clip_fraction        | 0.0474       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.366       |\n",
      "|    explained_variance   | 0.539        |\n",
      "|    learning_rate        | 0.000189     |\n",
      "|    loss                 | 1.24         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0106      |\n",
      "|    value_loss           | 3.25         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=3.40 +/- 2.87\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.4          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031369918 |\n",
      "|    clip_fraction        | 0.0378       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.319       |\n",
      "|    explained_variance   | 0.55         |\n",
      "|    learning_rate        | 0.000183     |\n",
      "|    loss                 | 2.22         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.009       |\n",
      "|    value_loss           | 3.3          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.9      |\n",
      "| time/              |          |\n",
      "|    fps             | 3025     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.85         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3024         |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018073367 |\n",
      "|    clip_fraction        | 0.0264       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.29        |\n",
      "|    explained_variance   | 0.574        |\n",
      "|    learning_rate        | 0.000177     |\n",
      "|    loss                 | 0.934        |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00828     |\n",
      "|    value_loss           | 3.19         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=2.60 +/- 2.33\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 2.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001689263 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.259      |\n",
      "|    explained_variance   | 0.571       |\n",
      "|    learning_rate        | 0.000171    |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00807    |\n",
      "|    value_loss           | 3.13        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.95     |\n",
      "| time/              |          |\n",
      "|    fps             | 3026     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.1          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3029         |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016478288 |\n",
      "|    clip_fraction        | 0.0242       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.225       |\n",
      "|    explained_variance   | 0.569        |\n",
      "|    learning_rate        | 0.000165     |\n",
      "|    loss                 | 1.27         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00626     |\n",
      "|    value_loss           | 3.1          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.27         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3032         |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013298166 |\n",
      "|    clip_fraction        | 0.0184       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.201       |\n",
      "|    explained_variance   | 0.571        |\n",
      "|    learning_rate        | 0.000159     |\n",
      "|    loss                 | 1.93         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00598     |\n",
      "|    value_loss           | 3.14         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=4.20 +/- 2.40\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 4.2          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011962494 |\n",
      "|    clip_fraction        | 0.0165       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.177       |\n",
      "|    explained_variance   | 0.556        |\n",
      "|    learning_rate        | 0.000153     |\n",
      "|    loss                 | 1.07         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00539     |\n",
      "|    value_loss           | 3.12         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 3033     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.51         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3033         |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008660297 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.157       |\n",
      "|    explained_variance   | 0.57         |\n",
      "|    learning_rate        | 0.000146     |\n",
      "|    loss                 | 1.13         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00512     |\n",
      "|    value_loss           | 3.11         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=6.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 6.2          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 55000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006363393 |\n",
      "|    clip_fraction        | 0.0107       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.143       |\n",
      "|    explained_variance   | 0.571        |\n",
      "|    learning_rate        | 0.00014      |\n",
      "|    loss                 | 1.62         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00275     |\n",
      "|    value_loss           | 3.1          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.72     |\n",
      "| time/              |          |\n",
      "|    fps             | 3032     |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3024        |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000593763 |\n",
      "|    clip_fraction        | 0.00732     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.135      |\n",
      "|    explained_variance   | 0.574       |\n",
      "|    learning_rate        | 0.000134    |\n",
      "|    loss                 | 1.78        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00257    |\n",
      "|    value_loss           | 3.07        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.45         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3024         |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005891856 |\n",
      "|    clip_fraction        | 0.00937      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.127       |\n",
      "|    explained_variance   | 0.571        |\n",
      "|    learning_rate        | 0.000128     |\n",
      "|    loss                 | 2            |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00416     |\n",
      "|    value_loss           | 3.1          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=4.20 +/- 2.48\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 4.2           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 60000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00070432376 |\n",
      "|    clip_fraction        | 0.00981       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.128        |\n",
      "|    explained_variance   | 0.602         |\n",
      "|    learning_rate        | 0.000122      |\n",
      "|    loss                 | 1.3           |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.0028       |\n",
      "|    value_loss           | 2.75          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.75     |\n",
      "| time/              |          |\n",
      "|    fps             | 3023     |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.04         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3024         |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008834238 |\n",
      "|    clip_fraction        | 0.0109       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.117       |\n",
      "|    explained_variance   | 0.517        |\n",
      "|    learning_rate        | 0.000116     |\n",
      "|    loss                 | 1.61         |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00389     |\n",
      "|    value_loss           | 3.4          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=3.20 +/- 2.64\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.2          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 65000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005032873 |\n",
      "|    clip_fraction        | 0.00757      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.109       |\n",
      "|    explained_variance   | 0.598        |\n",
      "|    learning_rate        | 0.00011      |\n",
      "|    loss                 | 1.72         |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00268     |\n",
      "|    value_loss           | 2.84         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 3025     |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.15         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3025         |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004562265 |\n",
      "|    clip_fraction        | 0.00708      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.103       |\n",
      "|    explained_variance   | 0.548        |\n",
      "|    learning_rate        | 0.000103     |\n",
      "|    loss                 | 1.21         |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00255     |\n",
      "|    value_loss           | 3.13         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.63         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3025         |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005107091 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0978      |\n",
      "|    explained_variance   | 0.573        |\n",
      "|    learning_rate        | 9.72e-05     |\n",
      "|    loss                 | 1.61         |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00333     |\n",
      "|    value_loss           | 3.04         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=3.20 +/- 3.06\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.2          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 70000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002894489 |\n",
      "|    clip_fraction        | 0.00654      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0925      |\n",
      "|    explained_variance   | 0.558        |\n",
      "|    learning_rate        | 9.11e-05     |\n",
      "|    loss                 | 1.08         |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    value_loss           | 3.23         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.3      |\n",
      "| time/              |          |\n",
      "|    fps             | 3025     |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.02         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3025         |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004686216 |\n",
      "|    clip_fraction        | 0.00762      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0853      |\n",
      "|    explained_variance   | 0.607        |\n",
      "|    learning_rate        | 8.5e-05      |\n",
      "|    loss                 | 0.77         |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.0025      |\n",
      "|    value_loss           | 2.76         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=6.00 +/- 0.63\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 6             |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 75000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031599027 |\n",
      "|    clip_fraction        | 0.00659       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0836       |\n",
      "|    explained_variance   | 0.586         |\n",
      "|    learning_rate        | 7.88e-05      |\n",
      "|    loss                 | 1.28          |\n",
      "|    n_updates            | 360           |\n",
      "|    policy_gradient_loss | -0.00202      |\n",
      "|    value_loss           | 2.94          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 3025     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.98         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3026         |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002866464 |\n",
      "|    clip_fraction        | 0.00356      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0816      |\n",
      "|    explained_variance   | 0.565        |\n",
      "|    learning_rate        | 7.27e-05     |\n",
      "|    loss                 | 1.5          |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00139     |\n",
      "|    value_loss           | 3.14         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.68          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3027          |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 79872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031875187 |\n",
      "|    clip_fraction        | 0.00352       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0786       |\n",
      "|    explained_variance   | 0.585         |\n",
      "|    learning_rate        | 6.65e-05      |\n",
      "|    loss                 | 1.46          |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | -0.00181      |\n",
      "|    value_loss           | 2.94          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=2.80 +/- 2.79\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 2.8           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 80000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029280598 |\n",
      "|    clip_fraction        | 0.00444       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0756       |\n",
      "|    explained_variance   | 0.573         |\n",
      "|    learning_rate        | 6.04e-05      |\n",
      "|    loss                 | 1.39          |\n",
      "|    n_updates            | 390           |\n",
      "|    policy_gradient_loss | -0.0012       |\n",
      "|    value_loss           | 3.03          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 3026     |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.47          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3019          |\n",
      "|    iterations           | 41            |\n",
      "|    time_elapsed         | 27            |\n",
      "|    total_timesteps      | 83968         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025786203 |\n",
      "|    clip_fraction        | 0.00386       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0739       |\n",
      "|    explained_variance   | 0.557         |\n",
      "|    learning_rate        | 5.42e-05      |\n",
      "|    loss                 | 1.42          |\n",
      "|    n_updates            | 400           |\n",
      "|    policy_gradient_loss | -0.00132      |\n",
      "|    value_loss           | 3.09          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=3.00 +/- 2.45\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 85000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002907592 |\n",
      "|    clip_fraction        | 0.00552      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0715      |\n",
      "|    explained_variance   | 0.588        |\n",
      "|    learning_rate        | 4.81e-05     |\n",
      "|    loss                 | 2.02         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    value_loss           | 2.93         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.1      |\n",
      "| time/              |          |\n",
      "|    fps             | 3014     |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.57         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3010         |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001347992 |\n",
      "|    clip_fraction        | 0.000928     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0711      |\n",
      "|    explained_variance   | 0.586        |\n",
      "|    learning_rate        | 4.2e-05      |\n",
      "|    loss                 | 0.877        |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00094     |\n",
      "|    value_loss           | 2.87         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=2.60 +/- 2.24\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 2.6           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 90000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015188588 |\n",
      "|    clip_fraction        | 0.00083       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0689       |\n",
      "|    explained_variance   | 0.557         |\n",
      "|    learning_rate        | 3.58e-05      |\n",
      "|    loss                 | 1.29          |\n",
      "|    n_updates            | 430           |\n",
      "|    policy_gradient_loss | -0.00131      |\n",
      "|    value_loss           | 3.07          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.85     |\n",
      "| time/              |          |\n",
      "|    fps             | 3006     |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.19         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3004         |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.888527e-05 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0678      |\n",
      "|    explained_variance   | 0.588        |\n",
      "|    learning_rate        | 2.97e-05     |\n",
      "|    loss                 | 1.36         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.000597    |\n",
      "|    value_loss           | 2.94         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.11          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3003          |\n",
      "|    iterations           | 46            |\n",
      "|    time_elapsed         | 31            |\n",
      "|    total_timesteps      | 94208         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.3405175e-05 |\n",
      "|    clip_fraction        | 0.00127       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0662       |\n",
      "|    explained_variance   | 0.561         |\n",
      "|    learning_rate        | 2.35e-05      |\n",
      "|    loss                 | 1.88          |\n",
      "|    n_updates            | 450           |\n",
      "|    policy_gradient_loss | -0.00075      |\n",
      "|    value_loss           | 3.18          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=3.80 +/- 2.32\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.8          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 95000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.551638e-05 |\n",
      "|    clip_fraction        | 0.000635     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0665      |\n",
      "|    explained_variance   | 0.584        |\n",
      "|    learning_rate        | 1.74e-05     |\n",
      "|    loss                 | 1.95         |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.000826    |\n",
      "|    value_loss           | 2.99         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 3001     |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 96256    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.9           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2996          |\n",
      "|    iterations           | 48            |\n",
      "|    time_elapsed         | 32            |\n",
      "|    total_timesteps      | 98304         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2488228e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0648       |\n",
      "|    explained_variance   | 0.561         |\n",
      "|    learning_rate        | 1.12e-05      |\n",
      "|    loss                 | 1.39          |\n",
      "|    n_updates            | 470           |\n",
      "|    policy_gradient_loss | -0.000333     |\n",
      "|    value_loss           | 3.1           |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=3.80 +/- 2.71\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 3.8           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 100000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1148775e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0652       |\n",
      "|    explained_variance   | 0.558         |\n",
      "|    learning_rate        | 5.09e-06      |\n",
      "|    loss                 | 2.09          |\n",
      "|    n_updates            | 480           |\n",
      "|    policy_gradient_loss | -0.000193     |\n",
      "|    value_loss           | 3.17          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 2993     |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "Total reward after training with PPO: 6\n",
      "Task prize: 7\n",
      "Bidders' bids: [10, 5, 7, 12, 13, 6, 4, 6, 10, 14]\n",
      "Bidders' distances: [1, 1, 2, 2, 3, 3, 2, 2, 3, 3]\n",
      "Distance: 1\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3.common.logger import configure\n",
    "from envs.purchasing import PurchaseEnv\n",
    "import os\n",
    "\n",
    "# Create the environment\n",
    "env = PurchaseEnv(num_bidders=10)\n",
    "\n",
    "# Configure TensorBoard logging\n",
    "log_dir = \"./runs/baselines\"\n",
    "new_logger = configure(log_dir, [\"stdout\", \"tensorboard\"])\n",
    "\n",
    "# Define a learning rate schedule\n",
    "def linear_schedule(initial_value):\n",
    "    def func(progress_remaining):\n",
    "        return progress_remaining * initial_value\n",
    "    return func\n",
    "\n",
    "# Instantiate the agent with adaptive learning rate\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=0, n_steps=2048, batch_size=64, n_epochs=10, learning_rate=linear_schedule(3e-4), tensorboard_log=log_dir)\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# Create callbacks for saving models and evaluation\n",
    "checkpoint_callback = CheckpointCallback(save_freq=int(1e4), save_path=f\"{log_dir}/checkpoints\", name_prefix='ppo_model')\n",
    "eval_callback = EvalCallback(env, best_model_save_path=f\"{log_dir}/best_model\", log_path=log_dir, eval_freq=5000, deterministic=True, render=False)\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(1e5), callback=callback)\n",
    "\n",
    "log_dir = \"./runs/baselines\"\n",
    "model_path = os.path.join(log_dir, \"best_model.zip\")\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path)\n",
    "\n",
    "# Test the trained agent\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total reward after training with PPO:\", total_reward)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Update the environment with 20 bidders\n",
    "env = PurchaseEnv(num_bidders=10)\n",
    "\n",
    "# Run random policy\n",
    "random_rewards = []\n",
    "for _ in range(1000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    random_rewards.append(total_reward)\n",
    "\n",
    "# Run trained policy\n",
    "ppo_rewards = []\n",
    "for _ in range(1000):\n",
    "    obs, _= env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    ppo_rewards.append(total_reward)\n",
    "\n",
    "# Plot the results\n",
    "plt.hist(random_rewards, bins=20, alpha=0.5, label='Random Policy', color='blue', density=False)\n",
    "plt.hist(ppo_rewards, bins=20, alpha=0.5, label='Trained Policy', color='orange', density=False)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Total Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Total Rewards for Random and Trained Policies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task prize: 6\n",
      "Bidders' bids: [11, 10, 13, 8, 4, 6, 12, 10, 4, 12]\n",
      "Bidders' distances: [2, 3, 1, 2, 3, 1, 1, 1, 1, 1]\n",
      "Distance: 3\n",
      "\n",
      "With prize 6 and distance 3, chose action 3\n",
      "Got reward: 0\n"
     ]
    }
   ],
   "source": [
    "# Define a function to run the model and print the chosen action for different numbers of bidders\n",
    "env = PurchaseEnv(num_bidders=10)\n",
    "obs, _ = env.reset()\n",
    "action, _states = model.predict(obs)\n",
    "\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.render()\n",
    "\n",
    "print(f'\\nWith prize {obs[\"prize\"]} and distance {obs[\"distance\"]}, chose action {action}')\n",
    "\n",
    "print(f\"Got reward: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Task and Robot Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 8\n",
      "Reward: -1\n",
      "Task prize: 7\n",
      "Bidders' bids: [9, 0, 1, 0, 4, 14, 0, 14, 12, 13]\n",
      "Bidders' distances: [3, 1, 1, 2, 1, 2, 2, 2, 2, 3]\n",
      "Distance: 3\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class PurchaseEnv(gym.Env):\n",
    "\n",
    "    class Task:\n",
    "        def __init__(self):\n",
    "            self.prize = np.random.randint(5, 10)\n",
    "            self.type = np.random.randint(0, 2)\n",
    "\n",
    "    class Bidder:\n",
    "        def __init__(self):\n",
    "            self.bid = np.random.randint(0, 15)\n",
    "            self.distance = np.random.randint(1, 4)\n",
    "            self.type = np.random.randint(0, 2)\n",
    "\n",
    "    def __init__(self, num_bidders=10):\n",
    "        super(PurchaseEnv, self).__init__()\n",
    "        self.task = self.Task()\n",
    "        self.bidders = [self.Bidder() for _ in range(num_bidders)]\n",
    "        self.distance = np.random.randint(1, 4)\n",
    "        self.type = np.random.randint(0, 2)\n",
    "        self.action_space = spaces.Discrete(15)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'prize': spaces.Discrete(15),\n",
    "            'distance': spaces.Discrete(4),\n",
    "            'type': spaces.Discrete(2),\n",
    "        })\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.bidders = [self.Bidder() for _ in range(len(self.bidders))]\n",
    "        self.task = self.Task()\n",
    "        self.distance = np.random.randint(1, 4)\n",
    "        self.type = np.random.randint(0, 2)\n",
    "        initial_observation = {'prize': self.task.prize, 'distance': self.distance, 'type': self.type}\n",
    "        return initial_observation, {}\n",
    "    \n",
    "    def get_reward(self, action):\n",
    "\n",
    "        type_match = self.type == self.task.type\n",
    "\n",
    "        # filter out irrelevant bids\n",
    "        relevant_bids = []\n",
    "\n",
    "        for bid, bidder in zip([bidder.bid for bidder in self.bidders] + [action], self.bidders + [None]):\n",
    "            if type_match:\n",
    "                if bid <= (self.task.prize):\n",
    "                    relevant_bids.append(bid)\n",
    "            else:\n",
    "                if bid <= (self.task.prize - (bidder.distance if bidder else self.distance)):\n",
    "                    relevant_bids.append(bid)\n",
    "\n",
    "        assert relevant_bids, \"No relevant bids\"\n",
    "        \n",
    "        # determine reward\n",
    "        max_bid = max(relevant_bids)\n",
    "        if action == max_bid:\n",
    "            if type_match:\n",
    "                return self.task.prize\n",
    "            else:\n",
    "                return self.task.prize - self.distance\n",
    "        elif action > self.task.prize:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        terminated = True\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        reward = self.get_reward(action)\n",
    "        return {'prize': self.task.prize, 'distance': self.distance}, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Task prize: {self.task.prize}\")\n",
    "        print(f\"Bidders' bids: {[bidder.bid for bidder in self.bidders]}\")\n",
    "        print(f\"Bidders' distances: {[bidder.distance for bidder in self.bidders]}\")\n",
    "        print(f\"Distance: {self.distance}\")\n",
    "\n",
    "# Example usage\n",
    "env = PurchaseEnv()\n",
    "action = env.action_space.sample()\n",
    "print(\"Sampled action:\", action)\n",
    "\n",
    "obs = env.reset()\n",
    "# print(\"Initial observation:\", obs)\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "# print(\"Observation after step:\", obs)\n",
    "print(\"Reward:\", reward)\n",
    "# print(\"Done:\", done)\n",
    "# print(\"Info:\", info)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./runs/baselines\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.2      |\n",
      "| time/              |          |\n",
      "|    fps             | 5280     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.23        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3717        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026855508 |\n",
      "|    clip_fraction        | 0.587       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.69       |\n",
      "|    explained_variance   | -0.0174     |\n",
      "|    learning_rate        | 0.000294    |\n",
      "|    loss                 | 1.63        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0927     |\n",
      "|    value_loss           | 4.05        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=2.20 +/- 2.71\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | 2.2        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03212705 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.61      |\n",
      "|    explained_variance   | 0.036      |\n",
      "|    learning_rate        | 0.000288   |\n",
      "|    loss                 | 3.14       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.107     |\n",
      "|    value_loss           | 5.26       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devg/robopt/.venv/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 1.08     |\n",
      "| time/              |          |\n",
      "|    fps             | 3077     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.6         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3026        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027212795 |\n",
      "|    clip_fraction        | 0.664       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.0519      |\n",
      "|    learning_rate        | 0.000282    |\n",
      "|    loss                 | 2.98        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.106      |\n",
      "|    value_loss           | 6           |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=3.00 +/- 2.76\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 3           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027756479 |\n",
      "|    clip_fraction        | 0.603       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.33       |\n",
      "|    explained_variance   | 0.0783      |\n",
      "|    learning_rate        | 0.000275    |\n",
      "|    loss                 | 3.19        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.103      |\n",
      "|    value_loss           | 6.37        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 1.63     |\n",
      "| time/              |          |\n",
      "|    fps             | 2888     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 1.33       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2841       |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02574287 |\n",
      "|    clip_fraction        | 0.56       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.15      |\n",
      "|    explained_variance   | 0.115      |\n",
      "|    learning_rate        | 0.000269   |\n",
      "|    loss                 | 2.82       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0968    |\n",
      "|    value_loss           | 6.91       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 2           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2853        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032197554 |\n",
      "|    clip_fraction        | 0.493       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.94       |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.000263    |\n",
      "|    loss                 | 3.69        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0881     |\n",
      "|    value_loss           | 6.9         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=4.60 +/- 2.58\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 4.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030002713 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.000257    |\n",
      "|    loss                 | 3.04        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0768     |\n",
      "|    value_loss           | 6.63        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 2.26     |\n",
      "| time/              |          |\n",
      "|    fps             | 2855     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 2.56       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2861       |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03041262 |\n",
      "|    clip_fraction        | 0.333      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.52      |\n",
      "|    explained_variance   | 0.277      |\n",
      "|    learning_rate        | 0.000251   |\n",
      "|    loss                 | 2.75       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0632    |\n",
      "|    value_loss           | 6.32       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=3.80 +/- 3.37\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 3.8         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022982713 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.31        |\n",
      "|    learning_rate        | 0.000245    |\n",
      "|    loss                 | 3.21        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0586     |\n",
      "|    value_loss           | 5.86        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 2877     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.05        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2883        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026295947 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.342       |\n",
      "|    learning_rate        | 0.000239    |\n",
      "|    loss                 | 2.56        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 5.6         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2882        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019971728 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.396       |\n",
      "|    learning_rate        | 0.000232    |\n",
      "|    loss                 | 1.84        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    value_loss           | 4.99        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=5.00 +/- 1.79\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 5           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019090205 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.884      |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.000226    |\n",
      "|    loss                 | 1.86        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    value_loss           | 4.44        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.51     |\n",
      "| time/              |          |\n",
      "|    fps             | 2880     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2890        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010360936 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.745      |\n",
      "|    explained_variance   | 0.451       |\n",
      "|    learning_rate        | 0.00022     |\n",
      "|    loss                 | 1.3         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    value_loss           | 4.3         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=3.00 +/- 3.10\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1         |\n",
      "|    mean_reward          | 3         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 30000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0090287 |\n",
      "|    clip_fraction        | 0.113     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.633    |\n",
      "|    explained_variance   | 0.524     |\n",
      "|    learning_rate        | 0.000214  |\n",
      "|    loss                 | 1.49      |\n",
      "|    n_updates            | 140       |\n",
      "|    policy_gradient_loss | -0.021    |\n",
      "|    value_loss           | 3.75      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 2898     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.78         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2900         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069132345 |\n",
      "|    clip_fraction        | 0.0882       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.537       |\n",
      "|    explained_variance   | 0.503        |\n",
      "|    learning_rate        | 0.000208     |\n",
      "|    loss                 | 1.37         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0204      |\n",
      "|    value_loss           | 3.7          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2900        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005056572 |\n",
      "|    clip_fraction        | 0.0679      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.458      |\n",
      "|    explained_variance   | 0.495       |\n",
      "|    learning_rate        | 0.000202    |\n",
      "|    loss                 | 1.99        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 3.77        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=2.80 +/- 2.64\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 2.8          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 35000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036206897 |\n",
      "|    clip_fraction        | 0.0568       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.388       |\n",
      "|    explained_variance   | 0.531        |\n",
      "|    learning_rate        | 0.000196     |\n",
      "|    loss                 | 2.5          |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0132      |\n",
      "|    value_loss           | 3.53         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 2891     |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2898        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003963433 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.33       |\n",
      "|    explained_variance   | 0.544       |\n",
      "|    learning_rate        | 0.000189    |\n",
      "|    loss                 | 1.92        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 3.25        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=3.40 +/- 2.33\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.4          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013065682 |\n",
      "|    clip_fraction        | 0.023        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.291       |\n",
      "|    explained_variance   | 0.58         |\n",
      "|    learning_rate        | 0.000183     |\n",
      "|    loss                 | 1.51         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00707     |\n",
      "|    value_loss           | 3.05         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.48     |\n",
      "| time/              |          |\n",
      "|    fps             | 2886     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.75         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2887         |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021097977 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.259       |\n",
      "|    explained_variance   | 0.556        |\n",
      "|    learning_rate        | 0.000177     |\n",
      "|    loss                 | 1.7          |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00704     |\n",
      "|    value_loss           | 3.15         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=3.20 +/- 2.79\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 3.2         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001321423 |\n",
      "|    clip_fraction        | 0.0184      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.235      |\n",
      "|    explained_variance   | 0.565       |\n",
      "|    learning_rate        | 0.000171    |\n",
      "|    loss                 | 1.55        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00573    |\n",
      "|    value_loss           | 3.15        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.9      |\n",
      "| time/              |          |\n",
      "|    fps             | 2889     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.42         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2891         |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011776802 |\n",
      "|    clip_fraction        | 0.0139       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.212       |\n",
      "|    explained_variance   | 0.536        |\n",
      "|    learning_rate        | 0.000165     |\n",
      "|    loss                 | 1.46         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00511     |\n",
      "|    value_loss           | 3.3          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.11         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2896         |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008942466 |\n",
      "|    clip_fraction        | 0.0115       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.195       |\n",
      "|    explained_variance   | 0.568        |\n",
      "|    learning_rate        | 0.000159     |\n",
      "|    loss                 | 1.61         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00367     |\n",
      "|    value_loss           | 3.18         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=3.80 +/- 3.25\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.8          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010418843 |\n",
      "|    clip_fraction        | 0.0151       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.178       |\n",
      "|    explained_variance   | 0.581        |\n",
      "|    learning_rate        | 0.000153     |\n",
      "|    loss                 | 1.2          |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00427     |\n",
      "|    value_loss           | 3.1          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.38     |\n",
      "| time/              |          |\n",
      "|    fps             | 2896     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.64         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2886         |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010564842 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.162       |\n",
      "|    explained_variance   | 0.578        |\n",
      "|    learning_rate        | 0.000146     |\n",
      "|    loss                 | 2.11         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00512     |\n",
      "|    value_loss           | 2.96         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=4.80 +/- 1.17\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 4.8           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 55000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00087512063 |\n",
      "|    clip_fraction        | 0.0161        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.151        |\n",
      "|    explained_variance   | 0.575         |\n",
      "|    learning_rate        | 0.00014       |\n",
      "|    loss                 | 2.33          |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | -0.00507      |\n",
      "|    value_loss           | 3.1           |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.81     |\n",
      "| time/              |          |\n",
      "|    fps             | 2892     |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.92          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2896          |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 19            |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00066845247 |\n",
      "|    clip_fraction        | 0.00977       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.137        |\n",
      "|    explained_variance   | 0.603         |\n",
      "|    learning_rate        | 0.000134      |\n",
      "|    loss                 | 1.28          |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -0.0037       |\n",
      "|    value_loss           | 2.82          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.05          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2902          |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 20            |\n",
      "|    total_timesteps      | 59392         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00090757967 |\n",
      "|    clip_fraction        | 0.0149        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.125        |\n",
      "|    explained_variance   | 0.586         |\n",
      "|    learning_rate        | 0.000128      |\n",
      "|    loss                 | 2.2           |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -0.00448      |\n",
      "|    value_loss           | 2.91          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=3.20 +/- 2.93\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.2          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005181683 |\n",
      "|    clip_fraction        | 0.00713      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.115       |\n",
      "|    explained_variance   | 0.561        |\n",
      "|    learning_rate        | 0.000122     |\n",
      "|    loss                 | 1.19         |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    value_loss           | 3.14         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 2906     |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.17          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2911          |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 21            |\n",
      "|    total_timesteps      | 63488         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00065164047 |\n",
      "|    clip_fraction        | 0.0121        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.105        |\n",
      "|    explained_variance   | 0.554         |\n",
      "|    learning_rate        | 0.000116      |\n",
      "|    loss                 | 1.01          |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -0.00452      |\n",
      "|    value_loss           | 3.19          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=4.60 +/- 2.58\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 4.6          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 65000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004369716 |\n",
      "|    clip_fraction        | 0.0064       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0984      |\n",
      "|    explained_variance   | 0.556        |\n",
      "|    learning_rate        | 0.00011      |\n",
      "|    loss                 | 1.49         |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00182     |\n",
      "|    value_loss           | 3.2          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.86     |\n",
      "| time/              |          |\n",
      "|    fps             | 2915     |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.09          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2920          |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 23            |\n",
      "|    total_timesteps      | 67584         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039883587 |\n",
      "|    clip_fraction        | 0.00601       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0934       |\n",
      "|    explained_variance   | 0.579         |\n",
      "|    learning_rate        | 0.000103      |\n",
      "|    loss                 | 1.85          |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | -0.00203      |\n",
      "|    value_loss           | 3.05          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.38          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2925          |\n",
      "|    iterations           | 34            |\n",
      "|    time_elapsed         | 23            |\n",
      "|    total_timesteps      | 69632         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00022094222 |\n",
      "|    clip_fraction        | 0.004         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0893       |\n",
      "|    explained_variance   | 0.561         |\n",
      "|    learning_rate        | 9.72e-05      |\n",
      "|    loss                 | 1.35          |\n",
      "|    n_updates            | 330           |\n",
      "|    policy_gradient_loss | -0.00169      |\n",
      "|    value_loss           | 3.13          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=3.40 +/- 2.94\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.4          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 70000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004314135 |\n",
      "|    clip_fraction        | 0.0061       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0852      |\n",
      "|    explained_variance   | 0.555        |\n",
      "|    learning_rate        | 9.11e-05     |\n",
      "|    loss                 | 1.82         |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00208     |\n",
      "|    value_loss           | 3.11         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.09     |\n",
      "| time/              |          |\n",
      "|    fps             | 2927     |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.8           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2931          |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 25            |\n",
      "|    total_timesteps      | 73728         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00046830176 |\n",
      "|    clip_fraction        | 0.00869       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0798       |\n",
      "|    explained_variance   | 0.534         |\n",
      "|    learning_rate        | 8.5e-05       |\n",
      "|    loss                 | 2.14          |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | -0.0031       |\n",
      "|    value_loss           | 3.23          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=5.00 +/- 0.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 5           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 75000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000250242 |\n",
      "|    clip_fraction        | 0.00439     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0775     |\n",
      "|    explained_variance   | 0.598       |\n",
      "|    learning_rate        | 7.88e-05    |\n",
      "|    loss                 | 1.31        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00189    |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.95     |\n",
      "| time/              |          |\n",
      "|    fps             | 2934     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.81          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2936          |\n",
      "|    iterations           | 38            |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 77824         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033851515 |\n",
      "|    clip_fraction        | 0.00542       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0761       |\n",
      "|    explained_variance   | 0.57          |\n",
      "|    learning_rate        | 7.27e-05      |\n",
      "|    loss                 | 1.02          |\n",
      "|    n_updates            | 370           |\n",
      "|    policy_gradient_loss | -0.00276      |\n",
      "|    value_loss           | 2.92          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.14          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2937          |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 27            |\n",
      "|    total_timesteps      | 79872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032428018 |\n",
      "|    clip_fraction        | 0.00522       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0721       |\n",
      "|    explained_variance   | 0.582         |\n",
      "|    learning_rate        | 6.65e-05      |\n",
      "|    loss                 | 1.33          |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | -0.00181      |\n",
      "|    value_loss           | 2.9           |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=4.00 +/- 2.45\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 4             |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 80000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00042200307 |\n",
      "|    clip_fraction        | 0.00796       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0654       |\n",
      "|    explained_variance   | 0.571         |\n",
      "|    learning_rate        | 6.04e-05      |\n",
      "|    loss                 | 1.1           |\n",
      "|    n_updates            | 390           |\n",
      "|    policy_gradient_loss | -0.00375      |\n",
      "|    value_loss           | 3.07          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.98     |\n",
      "| time/              |          |\n",
      "|    fps             | 2940     |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.92          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2943          |\n",
      "|    iterations           | 41            |\n",
      "|    time_elapsed         | 28            |\n",
      "|    total_timesteps      | 83968         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015748892 |\n",
      "|    clip_fraction        | 0.00273       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.065        |\n",
      "|    explained_variance   | 0.578         |\n",
      "|    learning_rate        | 5.42e-05      |\n",
      "|    loss                 | 1.59          |\n",
      "|    n_updates            | 400           |\n",
      "|    policy_gradient_loss | -0.0013       |\n",
      "|    value_loss           | 2.96          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=4.40 +/- 2.42\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 4.4           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 85000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028698565 |\n",
      "|    clip_fraction        | 0.00469       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0649       |\n",
      "|    explained_variance   | 0.566         |\n",
      "|    learning_rate        | 4.81e-05      |\n",
      "|    loss                 | 1.69          |\n",
      "|    n_updates            | 410           |\n",
      "|    policy_gradient_loss | -0.00151      |\n",
      "|    value_loss           | 3.08          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.14     |\n",
      "| time/              |          |\n",
      "|    fps             | 2944     |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.83         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2946         |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001480561 |\n",
      "|    clip_fraction        | 0.00171      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0639      |\n",
      "|    explained_variance   | 0.569        |\n",
      "|    learning_rate        | 4.2e-05      |\n",
      "|    loss                 | 1.23         |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    value_loss           | 3.02         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=3.40 +/- 2.94\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 3.4           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 90000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021170778 |\n",
      "|    clip_fraction        | 0.00337       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0638       |\n",
      "|    explained_variance   | 0.571         |\n",
      "|    learning_rate        | 3.58e-05      |\n",
      "|    loss                 | 1.41          |\n",
      "|    n_updates            | 430           |\n",
      "|    policy_gradient_loss | -0.00153      |\n",
      "|    value_loss           | 3.07          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 2947     |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.29         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2941         |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001928889 |\n",
      "|    clip_fraction        | 0.00234      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0656      |\n",
      "|    explained_variance   | 0.535        |\n",
      "|    learning_rate        | 2.97e-05     |\n",
      "|    loss                 | 1.24         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.0018      |\n",
      "|    value_loss           | 3.36         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 4.15          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2939          |\n",
      "|    iterations           | 46            |\n",
      "|    time_elapsed         | 32            |\n",
      "|    total_timesteps      | 94208         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010148765 |\n",
      "|    clip_fraction        | 0.000488      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0638       |\n",
      "|    explained_variance   | 0.572         |\n",
      "|    learning_rate        | 2.35e-05      |\n",
      "|    loss                 | 1.96          |\n",
      "|    n_updates            | 450           |\n",
      "|    policy_gradient_loss | -0.000722     |\n",
      "|    value_loss           | 3.07          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=2.40 +/- 2.58\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 2.4           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 95000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014871074 |\n",
      "|    clip_fraction        | 0.0021        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0605       |\n",
      "|    explained_variance   | 0.576         |\n",
      "|    learning_rate        | 1.74e-05      |\n",
      "|    loss                 | 1.29          |\n",
      "|    n_updates            | 460           |\n",
      "|    policy_gradient_loss | -0.0016       |\n",
      "|    value_loss           | 2.98          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4        |\n",
      "| time/              |          |\n",
      "|    fps             | 2941     |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 96256    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.81         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2943         |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.578678e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0569      |\n",
      "|    explained_variance   | 0.588        |\n",
      "|    learning_rate        | 1.12e-05     |\n",
      "|    loss                 | 1.64         |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000447    |\n",
      "|    value_loss           | 2.85         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=5.00 +/- 2.83\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 5             |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 100000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7852395e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0567       |\n",
      "|    explained_variance   | 0.599         |\n",
      "|    learning_rate        | 5.09e-06      |\n",
      "|    loss                 | 1.47          |\n",
      "|    n_updates            | 480           |\n",
      "|    policy_gradient_loss | -0.000487     |\n",
      "|    value_loss           | 2.84          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.87     |\n",
      "| time/              |          |\n",
      "|    fps             | 2945     |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "Total reward after training with PPO: 4\n",
      "Task prize: 6\n",
      "Bidders' bids: [11, 7, 7, 14, 2, 14, 14, 9, 8, 14]\n",
      "Bidders' distances: [3, 1, 3, 3, 3, 1, 1, 2, 3, 2]\n",
      "Distance: 2\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3.common.logger import configure\n",
    "from envs.purchasing import PurchaseEnv\n",
    "import os\n",
    "\n",
    "# Create the environment\n",
    "env = PurchaseEnv(num_bidders=10)\n",
    "\n",
    "# Configure TensorBoard logging\n",
    "log_dir = \"./runs/baselines\"\n",
    "new_logger = configure(log_dir, [\"stdout\", \"tensorboard\"])\n",
    "\n",
    "# Define a learning rate schedule\n",
    "def linear_schedule(initial_value):\n",
    "    def func(progress_remaining):\n",
    "        return progress_remaining * initial_value\n",
    "    return func\n",
    "\n",
    "# Instantiate the agent with adaptive learning rate\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=0, n_steps=2048, batch_size=64, n_epochs=10, learning_rate=linear_schedule(3e-4), tensorboard_log=log_dir)\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# Create callbacks for saving models and evaluation\n",
    "checkpoint_callback = CheckpointCallback(save_freq=int(1e4), save_path=f\"{log_dir}/checkpoints\", name_prefix='ppo_model')\n",
    "eval_callback = EvalCallback(env, best_model_save_path=f\"{log_dir}/best_model\", log_path=log_dir, eval_freq=5000, deterministic=True, render=False)\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(1e5), callback=callback)\n",
    "\n",
    "log_dir = \"./runs/baselines\"\n",
    "model_path = os.path.join(log_dir, \"best_model.zip\")\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path)\n",
    "\n",
    "# Test the trained agent\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total reward after training with PPO:\", total_reward)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Update the environment with 20 bidders\n",
    "env = PurchaseEnv(num_bidders=20)\n",
    "\n",
    "# Run random policy\n",
    "random_rewards = []\n",
    "for _ in range(1000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    random_rewards.append(total_reward)\n",
    "\n",
    "# Run trained policy\n",
    "ppo_rewards = []\n",
    "for _ in range(1000):\n",
    "    obs, _= env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    ppo_rewards.append(total_reward)\n",
    "\n",
    "# Plot the results\n",
    "plt.hist(random_rewards, bins=20, alpha=0.5, label='Random Policy', color='blue', density=False)\n",
    "plt.hist(ppo_rewards, bins=20, alpha=0.5, label='Trained Policy', color='orange', density=False)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Total Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Total Rewards for Random and Trained Policies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
