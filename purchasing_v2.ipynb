{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task prize: 8\n",
      "Bidders' bids: [0, 12, 9, 11, 3, 10, 0, 8, 12, 1]\n",
      "Bidders' distances: [1, 3, 1, 1, 1, 1, 1, 3, 3, 1]\n",
      "Distance: 2\n"
     ]
    }
   ],
   "source": [
    "from envs.purchasing import PurchaseEnv\n",
    "\n",
    "env = PurchaseEnv()\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task prize: 9\n",
      "Bidders' bids: [9, 11, 2, 10, 11, 2, 3, 8, 12, 5]\n",
      "Bidders' distances: [2, 1, 2, 2, 2, 3, 3, 1, 2, 1]\n",
      "Distance: 2\n",
      "Action: 7\n",
      "Reward: 0\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "log_dir = \"./runs/baselines\"\n",
    "model_path = os.path.join(log_dir, \"best_model.zip\")\n",
    "\n",
    "# Load the model\n",
    "model = PPO.load(model_path, env=env)\n",
    "\n",
    "# Example usage of the model to make predictions\n",
    "# Assuming you have some input data in a variable called `input_data`\n",
    "# predictions = model.predict(input_data)\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# write the env step \n",
    "action, _states = model.predict(obs)\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "env.render()\n",
    "\n",
    "# print the action and the reward\n",
    "print(\"Action:\", action)\n",
    "print(\"Reward:\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## things that would be cool to demonstrate\n",
    "\n",
    "* performs better against adversarial bidding\n",
    "* pseudo-guarantees optimality\n",
    "* model inference time is quicker than calculating directly for large inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./runs/baselines/multiagent\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.26     |\n",
      "| time/              |          |\n",
      "|    fps             | 2321     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.26        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1931        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032792028 |\n",
      "|    clip_fraction        | 0.618       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.68       |\n",
      "|    explained_variance   | -0.0461     |\n",
      "|    learning_rate        | 0.000294    |\n",
      "|    loss                 | 1.63        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0926     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=3.60 +/- 2.33\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | 3.6        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03209556 |\n",
      "|    clip_fraction        | 0.584      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.6       |\n",
      "|    explained_variance   | 0.0203     |\n",
      "|    learning_rate        | 0.000288   |\n",
      "|    loss                 | 1.06       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0955    |\n",
      "|    value_loss           | 3.22       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devg/robopt/.venv/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 1871     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1837        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032754444 |\n",
      "|    clip_fraction        | 0.581       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.0263      |\n",
      "|    learning_rate        | 0.000282    |\n",
      "|    loss                 | 2.19        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0967     |\n",
      "|    value_loss           | 3.84        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=5.40 +/- 1.50\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | 5.4        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03449519 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.3       |\n",
      "|    explained_variance   | 0.0646     |\n",
      "|    learning_rate        | 0.000275   |\n",
      "|    loss                 | 2.7        |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.105     |\n",
      "|    value_loss           | 5.56       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 1825     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 1.94       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1820       |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02649558 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.08      |\n",
      "|    explained_variance   | 0.103      |\n",
      "|    learning_rate        | 0.000269   |\n",
      "|    loss                 | 3.09       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.109     |\n",
      "|    value_loss           | 6.02       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 2.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1817        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027225025 |\n",
      "|    clip_fraction        | 0.656       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.83       |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.000263    |\n",
      "|    loss                 | 3.44        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.106      |\n",
      "|    value_loss           | 6.7         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=3.60 +/- 3.20\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 3.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031410404 |\n",
      "|    clip_fraction        | 0.562       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.000257    |\n",
      "|    loss                 | 3.64        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0946     |\n",
      "|    value_loss           | 7.06        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 2.4      |\n",
      "| time/              |          |\n",
      "|    fps             | 1815     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.1         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1806        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028597843 |\n",
      "|    clip_fraction        | 0.534       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.000251    |\n",
      "|    loss                 | 3.23        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0871     |\n",
      "|    value_loss           | 6.78        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=5.00 +/- 0.63\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 5           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040398955 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.986      |\n",
      "|    explained_variance   | 0.299       |\n",
      "|    learning_rate        | 0.000245    |\n",
      "|    loss                 | 2.42        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.069      |\n",
      "|    value_loss           | 5.97        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.24     |\n",
      "| time/              |          |\n",
      "|    fps             | 1788     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 2.93       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1785       |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03376986 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.726     |\n",
      "|    explained_variance   | 0.346      |\n",
      "|    learning_rate        | 0.000239   |\n",
      "|    loss                 | 2.66       |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0487    |\n",
      "|    value_loss           | 5.54       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.25        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1785        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010409707 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.546      |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.000232    |\n",
      "|    loss                 | 1.95        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    value_loss           | 4.81        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=2.00 +/- 2.45\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 2           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004990856 |\n",
      "|    clip_fraction        | 0.0733      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.000226    |\n",
      "|    loss                 | 1.83        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 4.42        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.55     |\n",
      "| time/              |          |\n",
      "|    fps             | 1785     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.32         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1785         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049111242 |\n",
      "|    clip_fraction        | 0.0804       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.367       |\n",
      "|    explained_variance   | 0.464        |\n",
      "|    learning_rate        | 0.00022      |\n",
      "|    loss                 | 1.93         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0154      |\n",
      "|    value_loss           | 4.19         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=1.40 +/- 2.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 1.4          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030156411 |\n",
      "|    clip_fraction        | 0.0536       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.301       |\n",
      "|    explained_variance   | 0.477        |\n",
      "|    learning_rate        | 0.000214     |\n",
      "|    loss                 | 2.32         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0113      |\n",
      "|    value_loss           | 3.94         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.93     |\n",
      "| time/              |          |\n",
      "|    fps             | 1784     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.15         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1785         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018037979 |\n",
      "|    clip_fraction        | 0.0389       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.251       |\n",
      "|    explained_variance   | 0.476        |\n",
      "|    learning_rate        | 0.000208     |\n",
      "|    loss                 | 1.91         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00792     |\n",
      "|    value_loss           | 3.89         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.72         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1786         |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021206003 |\n",
      "|    clip_fraction        | 0.0323       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.214       |\n",
      "|    explained_variance   | 0.496        |\n",
      "|    learning_rate        | 0.000202     |\n",
      "|    loss                 | 1.66         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00626     |\n",
      "|    value_loss           | 3.79         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=3.60 +/- 3.20\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.6          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 35000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012277705 |\n",
      "|    clip_fraction        | 0.0251       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.176       |\n",
      "|    explained_variance   | 0.502        |\n",
      "|    learning_rate        | 0.000196     |\n",
      "|    loss                 | 2.01         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00482     |\n",
      "|    value_loss           | 3.71         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.27     |\n",
      "| time/              |          |\n",
      "|    fps             | 1784     |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.2          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1782         |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010133423 |\n",
      "|    clip_fraction        | 0.0229       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.148       |\n",
      "|    explained_variance   | 0.503        |\n",
      "|    learning_rate        | 0.000189     |\n",
      "|    loss                 | 2.23         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00514     |\n",
      "|    value_loss           | 3.84         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=2.80 +/- 1.72\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 2.8           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 40000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00065169344 |\n",
      "|    clip_fraction        | 0.00991       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.129        |\n",
      "|    explained_variance   | 0.494         |\n",
      "|    learning_rate        | 0.000183      |\n",
      "|    loss                 | 1.56          |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.0024       |\n",
      "|    value_loss           | 3.71          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.65     |\n",
      "| time/              |          |\n",
      "|    fps             | 1782     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.93          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1782          |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 24            |\n",
      "|    total_timesteps      | 43008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040466146 |\n",
      "|    clip_fraction        | 0.00869       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.112        |\n",
      "|    explained_variance   | 0.516         |\n",
      "|    learning_rate        | 0.000177      |\n",
      "|    loss                 | 1.53          |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | -0.00134      |\n",
      "|    value_loss           | 3.66          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=3.60 +/- 2.24\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.6          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 45000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005464676 |\n",
      "|    clip_fraction        | 0.00889      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.104       |\n",
      "|    explained_variance   | 0.509        |\n",
      "|    learning_rate        | 0.000171     |\n",
      "|    loss                 | 2.12         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00379     |\n",
      "|    value_loss           | 3.82         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.56     |\n",
      "| time/              |          |\n",
      "|    fps             | 1782     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.78          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1782          |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039315154 |\n",
      "|    clip_fraction        | 0.00723       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0991       |\n",
      "|    explained_variance   | 0.506         |\n",
      "|    learning_rate        | 0.000165      |\n",
      "|    loss                 | 1.57          |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.00249      |\n",
      "|    value_loss           | 3.69          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.86         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1781         |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004155114 |\n",
      "|    clip_fraction        | 0.00581      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0894      |\n",
      "|    explained_variance   | 0.499        |\n",
      "|    learning_rate        | 0.000159     |\n",
      "|    loss                 | 2.08         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 3.75         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=4.40 +/- 3.01\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 4.4          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003245879 |\n",
      "|    clip_fraction        | 0.00513      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0824      |\n",
      "|    explained_variance   | 0.47         |\n",
      "|    learning_rate        | 0.000153     |\n",
      "|    loss                 | 2.12         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    value_loss           | 3.92         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.89     |\n",
      "| time/              |          |\n",
      "|    fps             | 1781     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.14         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1782         |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002436935 |\n",
      "|    clip_fraction        | 0.0041       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0743      |\n",
      "|    explained_variance   | 0.531        |\n",
      "|    learning_rate        | 0.000146     |\n",
      "|    loss                 | 1.77         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    value_loss           | 3.67         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=3.80 +/- 2.32\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 3.8           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 55000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037806472 |\n",
      "|    clip_fraction        | 0.00518       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0686       |\n",
      "|    explained_variance   | 0.512         |\n",
      "|    learning_rate        | 0.00014       |\n",
      "|    loss                 | 1.87          |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | -0.00164      |\n",
      "|    value_loss           | 3.61          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.71     |\n",
      "| time/              |          |\n",
      "|    fps             | 1782     |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.89         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1780         |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003225531 |\n",
      "|    clip_fraction        | 0.00605      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.064       |\n",
      "|    explained_variance   | 0.517        |\n",
      "|    learning_rate        | 0.000134     |\n",
      "|    loss                 | 1.59         |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    value_loss           | 3.67         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.62         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1780         |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003457396 |\n",
      "|    clip_fraction        | 0.00708      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0575      |\n",
      "|    explained_variance   | 0.489        |\n",
      "|    learning_rate        | 0.000128     |\n",
      "|    loss                 | 1.63         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00254     |\n",
      "|    value_loss           | 3.92         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=4.40 +/- 1.50\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 4.4          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002701518 |\n",
      "|    clip_fraction        | 0.00454      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0509      |\n",
      "|    explained_variance   | 0.497        |\n",
      "|    learning_rate        | 0.000122     |\n",
      "|    loss                 | 2.24         |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    value_loss           | 3.9          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.51     |\n",
      "| time/              |          |\n",
      "|    fps             | 1780     |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.53          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1781          |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 35            |\n",
      "|    total_timesteps      | 63488         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039080685 |\n",
      "|    clip_fraction        | 0.00752       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0449       |\n",
      "|    explained_variance   | 0.492         |\n",
      "|    learning_rate        | 0.000116      |\n",
      "|    loss                 | 1.62          |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -0.00197      |\n",
      "|    value_loss           | 3.89          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=2.80 +/- 3.49\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 2.8           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 65000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014735584 |\n",
      "|    clip_fraction        | 0.00293       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0425       |\n",
      "|    explained_variance   | 0.531         |\n",
      "|    learning_rate        | 0.00011       |\n",
      "|    loss                 | 1.53          |\n",
      "|    n_updates            | 310           |\n",
      "|    policy_gradient_loss | -0.000784     |\n",
      "|    value_loss           | 3.64          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.71     |\n",
      "| time/              |          |\n",
      "|    fps             | 1780     |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.7           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1781          |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 37            |\n",
      "|    total_timesteps      | 67584         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020788124 |\n",
      "|    clip_fraction        | 0.00356       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0397       |\n",
      "|    explained_variance   | 0.495         |\n",
      "|    learning_rate        | 0.000103      |\n",
      "|    loss                 | 1.75          |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | -0.00121      |\n",
      "|    value_loss           | 3.82          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.27          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1781          |\n",
      "|    iterations           | 34            |\n",
      "|    time_elapsed         | 39            |\n",
      "|    total_timesteps      | 69632         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013958433 |\n",
      "|    clip_fraction        | 0.00254       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0358       |\n",
      "|    explained_variance   | 0.502         |\n",
      "|    learning_rate        | 9.72e-05      |\n",
      "|    loss                 | 1.83          |\n",
      "|    n_updates            | 330           |\n",
      "|    policy_gradient_loss | -0.000513     |\n",
      "|    value_loss           | 3.77          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=5.40 +/- 1.50\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 5.4           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 70000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012747478 |\n",
      "|    clip_fraction        | 0.00161       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0333       |\n",
      "|    explained_variance   | 0.5           |\n",
      "|    learning_rate        | 9.11e-05      |\n",
      "|    loss                 | 2.2           |\n",
      "|    n_updates            | 340           |\n",
      "|    policy_gradient_loss | -0.00065      |\n",
      "|    value_loss           | 3.75          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.58     |\n",
      "| time/              |          |\n",
      "|    fps             | 1781     |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.44          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1781          |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 41            |\n",
      "|    total_timesteps      | 73728         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019965085 |\n",
      "|    clip_fraction        | 0.00352       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.031        |\n",
      "|    explained_variance   | 0.508         |\n",
      "|    learning_rate        | 8.5e-05       |\n",
      "|    loss                 | 1.95          |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | -0.00103      |\n",
      "|    value_loss           | 3.7           |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=4.00 +/- 2.10\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 4            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 75000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.594497e-05 |\n",
      "|    clip_fraction        | 0.00137      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0315      |\n",
      "|    explained_variance   | 0.527        |\n",
      "|    learning_rate        | 7.88e-05     |\n",
      "|    loss                 | 2.05         |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.000519    |\n",
      "|    value_loss           | 3.5          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 1781     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 1              |\n",
      "|    ep_rew_mean          | 3.76           |\n",
      "| time/                   |                |\n",
      "|    fps                  | 1780           |\n",
      "|    iterations           | 38             |\n",
      "|    time_elapsed         | 43             |\n",
      "|    total_timesteps      | 77824          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000121895486 |\n",
      "|    clip_fraction        | 0.00151        |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.0296        |\n",
      "|    explained_variance   | 0.488          |\n",
      "|    learning_rate        | 7.27e-05       |\n",
      "|    loss                 | 1.69           |\n",
      "|    n_updates            | 370            |\n",
      "|    policy_gradient_loss | -0.000445      |\n",
      "|    value_loss           | 3.81           |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.86         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1781         |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.499054e-05 |\n",
      "|    clip_fraction        | 0.00122      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0281      |\n",
      "|    explained_variance   | 0.514        |\n",
      "|    learning_rate        | 6.65e-05     |\n",
      "|    loss                 | 1.93         |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.000398    |\n",
      "|    value_loss           | 3.75         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=3.60 +/- 3.20\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 3.6           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 80000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.7391876e-05 |\n",
      "|    clip_fraction        | 0.00107       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0257       |\n",
      "|    explained_variance   | 0.535         |\n",
      "|    learning_rate        | 6.04e-05      |\n",
      "|    loss                 | 1.63          |\n",
      "|    n_updates            | 390           |\n",
      "|    policy_gradient_loss | -0.000428     |\n",
      "|    value_loss           | 3.58          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.85     |\n",
      "| time/              |          |\n",
      "|    fps             | 1780     |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.68          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1781          |\n",
      "|    iterations           | 41            |\n",
      "|    time_elapsed         | 47            |\n",
      "|    total_timesteps      | 83968         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.6952733e-05 |\n",
      "|    clip_fraction        | 0.00137       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0244       |\n",
      "|    explained_variance   | 0.489         |\n",
      "|    learning_rate        | 5.42e-05      |\n",
      "|    loss                 | 1.64          |\n",
      "|    n_updates            | 400           |\n",
      "|    policy_gradient_loss | -0.000793     |\n",
      "|    value_loss           | 3.91          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=4.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 4.6          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 85000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.897267e-05 |\n",
      "|    clip_fraction        | 0.00117      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0236      |\n",
      "|    explained_variance   | 0.508        |\n",
      "|    learning_rate        | 4.81e-05     |\n",
      "|    loss                 | 1.86         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.000571    |\n",
      "|    value_loss           | 3.75         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.35     |\n",
      "| time/              |          |\n",
      "|    fps             | 1781     |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.29        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1781        |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 3.05851e-05 |\n",
      "|    clip_fraction        | 0.000537    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0232     |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 4.2e-05     |\n",
      "|    loss                 | 1.96        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00039    |\n",
      "|    value_loss           | 3.91        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=3.60 +/- 3.20\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 3.6           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 90000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9983657e-05 |\n",
      "|    clip_fraction        | 0.000342      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.022        |\n",
      "|    explained_variance   | 0.502         |\n",
      "|    learning_rate        | 3.58e-05      |\n",
      "|    loss                 | 2.18          |\n",
      "|    n_updates            | 430           |\n",
      "|    policy_gradient_loss | -0.000343     |\n",
      "|    value_loss           | 3.77          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.36     |\n",
      "| time/              |          |\n",
      "|    fps             | 1772     |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.57          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1769          |\n",
      "|    iterations           | 45            |\n",
      "|    time_elapsed         | 52            |\n",
      "|    total_timesteps      | 92160         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014314285 |\n",
      "|    clip_fraction        | 0.00254       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0206       |\n",
      "|    explained_variance   | 0.486         |\n",
      "|    learning_rate        | 2.97e-05      |\n",
      "|    loss                 | 1.79          |\n",
      "|    n_updates            | 440           |\n",
      "|    policy_gradient_loss | -0.00101      |\n",
      "|    value_loss           | 3.84          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.43          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1770          |\n",
      "|    iterations           | 46            |\n",
      "|    time_elapsed         | 53            |\n",
      "|    total_timesteps      | 94208         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.8077596e-05 |\n",
      "|    clip_fraction        | 0.000732      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0208       |\n",
      "|    explained_variance   | 0.494         |\n",
      "|    learning_rate        | 2.35e-05      |\n",
      "|    loss                 | 1.61          |\n",
      "|    n_updates            | 450           |\n",
      "|    policy_gradient_loss | -0.000522     |\n",
      "|    value_loss           | 3.87          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=1.60 +/- 1.96\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 1.6           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 95000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6454782e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0203       |\n",
      "|    explained_variance   | 0.504         |\n",
      "|    learning_rate        | 1.74e-05      |\n",
      "|    loss                 | 1.83          |\n",
      "|    n_updates            | 460           |\n",
      "|    policy_gradient_loss | -0.000127     |\n",
      "|    value_loss           | 3.73          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.2      |\n",
      "| time/              |          |\n",
      "|    fps             | 1770     |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 54       |\n",
      "|    total_timesteps | 96256    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.66          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1771          |\n",
      "|    iterations           | 48            |\n",
      "|    time_elapsed         | 55            |\n",
      "|    total_timesteps      | 98304         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0438416e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0205       |\n",
      "|    explained_variance   | 0.49          |\n",
      "|    learning_rate        | 1.12e-05      |\n",
      "|    loss                 | 2.29          |\n",
      "|    n_updates            | 470           |\n",
      "|    policy_gradient_loss | -0.000343     |\n",
      "|    value_loss           | 3.89          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=2.80 +/- 2.79\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 2.8           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 100000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2205943e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0196       |\n",
      "|    explained_variance   | 0.529         |\n",
      "|    learning_rate        | 5.09e-06      |\n",
      "|    loss                 | 1.84          |\n",
      "|    n_updates            | 480           |\n",
      "|    policy_gradient_loss | -0.000182     |\n",
      "|    value_loss           | 3.52          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.53     |\n",
      "| time/              |          |\n",
      "|    fps             | 1771     |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "Total reward after training with PPO: 4\n",
      "Task prize: 5\n",
      "Bidders' bids: [3, 3]\n",
      "Bidders' distances: [2, 3]\n",
      "Distance: 1\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3.common.logger import configure\n",
    "from envs.multiagent_purchasing import MultiAgent_PurchaseEnv\n",
    "import os\n",
    "\n",
    "# Create the environment\n",
    "env = MultiAgent_PurchaseEnv(num_bidders=2)\n",
    "\n",
    "# Configure TensorBoard logging\n",
    "log_dir = \"./runs/baselines/multiagent\"\n",
    "new_logger = configure(log_dir, [\"stdout\", \"tensorboard\"])\n",
    "\n",
    "# Define a learning rate schedule\n",
    "def linear_schedule(initial_value):\n",
    "    def func(progress_remaining):\n",
    "        return progress_remaining * initial_value\n",
    "    return func\n",
    "\n",
    "# Instantiate the agent with adaptive learning rate\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=0, n_steps=2048, batch_size=64, n_epochs=10, learning_rate=linear_schedule(3e-4), tensorboard_log=log_dir)\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# Create callbacks for saving models and evaluation\n",
    "checkpoint_callback = CheckpointCallback(save_freq=int(1e4), save_path=f\"{log_dir}/checkpoints\", name_prefix='ppo_model')\n",
    "eval_callback = EvalCallback(env, best_model_save_path=f\"{log_dir}/best_model\", log_path=log_dir, eval_freq=5000, deterministic=True, render=False)\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(1e5), callback=callback)\n",
    "\n",
    "model_path = os.path.join(log_dir, \"best_model.zip\")\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path)\n",
    "\n",
    "# Test the trained agent\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total reward after training with PPO:\", total_reward)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from envs.multiagent_purchasing import MultiAgent_PurchaseEnv\n",
    "\n",
    "# Update the environment with 20 bidders\n",
    "env = MultiAgent_PurchaseEnv(num_bidders=2)\n",
    "\n",
    "# Run random policy\n",
    "random_rewards = []\n",
    "for _ in range(1000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    random_rewards.append(total_reward)\n",
    "\n",
    "# Run trained policy\n",
    "ppo_rewards = []\n",
    "for _ in range(1000):\n",
    "    obs, _= env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    ppo_rewards.append(total_reward)\n",
    "\n",
    "# Plot the results\n",
    "plt.hist(random_rewards, bins=20, alpha=0.5, label='Random Policy', color='blue', density=False)\n",
    "plt.hist(ppo_rewards, bins=20, alpha=0.5, label='Trained Policy', color='orange', density=False)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Total Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Total Rewards for Random and Trained Policies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task prize: 8\n",
      "Bidders' bids: [5, 6, 5, 7]\n",
      "Bidders' distances: [3, 2, 3, 1]\n",
      "Distance: 2\n",
      "\n",
      "With prize 8 and distance 2, chose action 7\n",
      "Got reward: 6\n"
     ]
    }
   ],
   "source": [
    "# Define a function to run the model and print the chosen action for different numbers of bidders\n",
    "env = MultiAgent_PurchaseEnv(num_bidders=4)\n",
    "obs, _ = env.reset()\n",
    "action, _states = model.predict(obs)\n",
    "\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.render()\n",
    "\n",
    "print(f'\\nWith prize {obs[\"prize\"]} and distance {obs[\"distance\"]}, chose action {action}')\n",
    "\n",
    "print(f\"Got reward: {reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
