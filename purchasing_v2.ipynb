{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task prize: 8\n",
      "Bidders' bids: [0, 12, 9, 11, 3, 10, 0, 8, 12, 1]\n",
      "Bidders' distances: [1, 3, 1, 1, 1, 1, 1, 3, 3, 1]\n",
      "Distance: 2\n"
     ]
    }
   ],
   "source": [
    "from envs.purchasing import PurchaseEnv\n",
    "\n",
    "env = PurchaseEnv()\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task prize: 9\n",
      "Bidders' bids: [9, 11, 2, 10, 11, 2, 3, 8, 12, 5]\n",
      "Bidders' distances: [2, 1, 2, 2, 2, 3, 3, 1, 2, 1]\n",
      "Distance: 2\n",
      "Action: 7\n",
      "Reward: 0\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "log_dir = \"./runs/baselines\"\n",
    "model_path = os.path.join(log_dir, \"best_model.zip\")\n",
    "\n",
    "# Load the model\n",
    "model = PPO.load(model_path, env=env)\n",
    "\n",
    "# Example usage of the model to make predictions\n",
    "# Assuming you have some input data in a variable called `input_data`\n",
    "# predictions = model.predict(input_data)\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# write the env step \n",
    "action, _states = model.predict(obs)\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "env.render()\n",
    "\n",
    "# print the action and the reward\n",
    "print(\"Action:\", action)\n",
    "print(\"Reward:\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## things that would be cool to demonstrate\n",
    "\n",
    "* performs better against adversarial bidding\n",
    "* pseudo-guarantees optimality\n",
    "* model inference time is quicker than calculating directly for large inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./runs/baselines/multiagent\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.26     |\n",
      "| time/              |          |\n",
      "|    fps             | 2321     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.26        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1931        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032792028 |\n",
      "|    clip_fraction        | 0.618       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.68       |\n",
      "|    explained_variance   | -0.0461     |\n",
      "|    learning_rate        | 0.000294    |\n",
      "|    loss                 | 1.63        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0926     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=3.60 +/- 2.33\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | 3.6        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03209556 |\n",
      "|    clip_fraction        | 0.584      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.6       |\n",
      "|    explained_variance   | 0.0203     |\n",
      "|    learning_rate        | 0.000288   |\n",
      "|    loss                 | 1.06       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0955    |\n",
      "|    value_loss           | 3.22       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devg/robopt/.venv/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 1871     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1837        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032754444 |\n",
      "|    clip_fraction        | 0.581       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.0263      |\n",
      "|    learning_rate        | 0.000282    |\n",
      "|    loss                 | 2.19        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0967     |\n",
      "|    value_loss           | 3.84        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=5.40 +/- 1.50\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | 5.4        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03449519 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.3       |\n",
      "|    explained_variance   | 0.0646     |\n",
      "|    learning_rate        | 0.000275   |\n",
      "|    loss                 | 2.7        |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.105     |\n",
      "|    value_loss           | 5.56       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 1825     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 1.94       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1820       |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02649558 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.08      |\n",
      "|    explained_variance   | 0.103      |\n",
      "|    learning_rate        | 0.000269   |\n",
      "|    loss                 | 3.09       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.109     |\n",
      "|    value_loss           | 6.02       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 2.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1817        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027225025 |\n",
      "|    clip_fraction        | 0.656       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.83       |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.000263    |\n",
      "|    loss                 | 3.44        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.106      |\n",
      "|    value_loss           | 6.7         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=3.60 +/- 3.20\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 3.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031410404 |\n",
      "|    clip_fraction        | 0.562       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.000257    |\n",
      "|    loss                 | 3.64        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0946     |\n",
      "|    value_loss           | 7.06        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 2.4      |\n",
      "| time/              |          |\n",
      "|    fps             | 1815     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.1         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1806        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028597843 |\n",
      "|    clip_fraction        | 0.534       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.000251    |\n",
      "|    loss                 | 3.23        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0871     |\n",
      "|    value_loss           | 6.78        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=5.00 +/- 0.63\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 5           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040398955 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.986      |\n",
      "|    explained_variance   | 0.299       |\n",
      "|    learning_rate        | 0.000245    |\n",
      "|    loss                 | 2.42        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.069      |\n",
      "|    value_loss           | 5.97        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.24     |\n",
      "| time/              |          |\n",
      "|    fps             | 1788     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 2.93       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1785       |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03376986 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.726     |\n",
      "|    explained_variance   | 0.346      |\n",
      "|    learning_rate        | 0.000239   |\n",
      "|    loss                 | 2.66       |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0487    |\n",
      "|    value_loss           | 5.54       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.25        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1785        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010409707 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.546      |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.000232    |\n",
      "|    loss                 | 1.95        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    value_loss           | 4.81        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=2.00 +/- 2.45\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 2           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004990856 |\n",
      "|    clip_fraction        | 0.0733      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.000226    |\n",
      "|    loss                 | 1.83        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 4.42        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.55     |\n",
      "| time/              |          |\n",
      "|    fps             | 1785     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.32         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1785         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049111242 |\n",
      "|    clip_fraction        | 0.0804       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.367       |\n",
      "|    explained_variance   | 0.464        |\n",
      "|    learning_rate        | 0.00022      |\n",
      "|    loss                 | 1.93         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0154      |\n",
      "|    value_loss           | 4.19         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=1.40 +/- 2.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 1.4          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030156411 |\n",
      "|    clip_fraction        | 0.0536       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.301       |\n",
      "|    explained_variance   | 0.477        |\n",
      "|    learning_rate        | 0.000214     |\n",
      "|    loss                 | 2.32         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0113      |\n",
      "|    value_loss           | 3.94         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.93     |\n",
      "| time/              |          |\n",
      "|    fps             | 1784     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.15         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1785         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018037979 |\n",
      "|    clip_fraction        | 0.0389       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.251       |\n",
      "|    explained_variance   | 0.476        |\n",
      "|    learning_rate        | 0.000208     |\n",
      "|    loss                 | 1.91         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00792     |\n",
      "|    value_loss           | 3.89         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.72         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1786         |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021206003 |\n",
      "|    clip_fraction        | 0.0323       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.214       |\n",
      "|    explained_variance   | 0.496        |\n",
      "|    learning_rate        | 0.000202     |\n",
      "|    loss                 | 1.66         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00626     |\n",
      "|    value_loss           | 3.79         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=3.60 +/- 3.20\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.6          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 35000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012277705 |\n",
      "|    clip_fraction        | 0.0251       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.176       |\n",
      "|    explained_variance   | 0.502        |\n",
      "|    learning_rate        | 0.000196     |\n",
      "|    loss                 | 2.01         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00482     |\n",
      "|    value_loss           | 3.71         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.27     |\n",
      "| time/              |          |\n",
      "|    fps             | 1784     |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.2          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1782         |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010133423 |\n",
      "|    clip_fraction        | 0.0229       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.148       |\n",
      "|    explained_variance   | 0.503        |\n",
      "|    learning_rate        | 0.000189     |\n",
      "|    loss                 | 2.23         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00514     |\n",
      "|    value_loss           | 3.84         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=2.80 +/- 1.72\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 2.8           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 40000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00065169344 |\n",
      "|    clip_fraction        | 0.00991       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.129        |\n",
      "|    explained_variance   | 0.494         |\n",
      "|    learning_rate        | 0.000183      |\n",
      "|    loss                 | 1.56          |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.0024       |\n",
      "|    value_loss           | 3.71          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.65     |\n",
      "| time/              |          |\n",
      "|    fps             | 1782     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.93          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1782          |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 24            |\n",
      "|    total_timesteps      | 43008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040466146 |\n",
      "|    clip_fraction        | 0.00869       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.112        |\n",
      "|    explained_variance   | 0.516         |\n",
      "|    learning_rate        | 0.000177      |\n",
      "|    loss                 | 1.53          |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | -0.00134      |\n",
      "|    value_loss           | 3.66          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=3.60 +/- 2.24\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 3.6          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 45000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005464676 |\n",
      "|    clip_fraction        | 0.00889      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.104       |\n",
      "|    explained_variance   | 0.509        |\n",
      "|    learning_rate        | 0.000171     |\n",
      "|    loss                 | 2.12         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00379     |\n",
      "|    value_loss           | 3.82         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.56     |\n",
      "| time/              |          |\n",
      "|    fps             | 1782     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.78          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1782          |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039315154 |\n",
      "|    clip_fraction        | 0.00723       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0991       |\n",
      "|    explained_variance   | 0.506         |\n",
      "|    learning_rate        | 0.000165      |\n",
      "|    loss                 | 1.57          |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.00249      |\n",
      "|    value_loss           | 3.69          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.86         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1781         |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004155114 |\n",
      "|    clip_fraction        | 0.00581      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0894      |\n",
      "|    explained_variance   | 0.499        |\n",
      "|    learning_rate        | 0.000159     |\n",
      "|    loss                 | 2.08         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 3.75         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=4.40 +/- 3.01\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 4.4          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003245879 |\n",
      "|    clip_fraction        | 0.00513      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0824      |\n",
      "|    explained_variance   | 0.47         |\n",
      "|    learning_rate        | 0.000153     |\n",
      "|    loss                 | 2.12         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    value_loss           | 3.92         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.89     |\n",
      "| time/              |          |\n",
      "|    fps             | 1781     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 4.14         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1782         |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002436935 |\n",
      "|    clip_fraction        | 0.0041       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0743      |\n",
      "|    explained_variance   | 0.531        |\n",
      "|    learning_rate        | 0.000146     |\n",
      "|    loss                 | 1.77         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    value_loss           | 3.67         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=3.80 +/- 2.32\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 3.8           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 55000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037806472 |\n",
      "|    clip_fraction        | 0.00518       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0686       |\n",
      "|    explained_variance   | 0.512         |\n",
      "|    learning_rate        | 0.00014       |\n",
      "|    loss                 | 1.87          |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | -0.00164      |\n",
      "|    value_loss           | 3.61          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.71     |\n",
      "| time/              |          |\n",
      "|    fps             | 1782     |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.89         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1780         |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003225531 |\n",
      "|    clip_fraction        | 0.00605      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.064       |\n",
      "|    explained_variance   | 0.517        |\n",
      "|    learning_rate        | 0.000134     |\n",
      "|    loss                 | 1.59         |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    value_loss           | 3.67         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.62         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1780         |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003457396 |\n",
      "|    clip_fraction        | 0.00708      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0575      |\n",
      "|    explained_variance   | 0.489        |\n",
      "|    learning_rate        | 0.000128     |\n",
      "|    loss                 | 1.63         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00254     |\n",
      "|    value_loss           | 3.92         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=4.40 +/- 1.50\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 4.4          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002701518 |\n",
      "|    clip_fraction        | 0.00454      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0509      |\n",
      "|    explained_variance   | 0.497        |\n",
      "|    learning_rate        | 0.000122     |\n",
      "|    loss                 | 2.24         |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    value_loss           | 3.9          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.51     |\n",
      "| time/              |          |\n",
      "|    fps             | 1780     |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.53          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1781          |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 35            |\n",
      "|    total_timesteps      | 63488         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039080685 |\n",
      "|    clip_fraction        | 0.00752       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0449       |\n",
      "|    explained_variance   | 0.492         |\n",
      "|    learning_rate        | 0.000116      |\n",
      "|    loss                 | 1.62          |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -0.00197      |\n",
      "|    value_loss           | 3.89          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=2.80 +/- 3.49\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 2.8           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 65000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014735584 |\n",
      "|    clip_fraction        | 0.00293       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0425       |\n",
      "|    explained_variance   | 0.531         |\n",
      "|    learning_rate        | 0.00011       |\n",
      "|    loss                 | 1.53          |\n",
      "|    n_updates            | 310           |\n",
      "|    policy_gradient_loss | -0.000784     |\n",
      "|    value_loss           | 3.64          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.71     |\n",
      "| time/              |          |\n",
      "|    fps             | 1780     |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.7           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1781          |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 37            |\n",
      "|    total_timesteps      | 67584         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020788124 |\n",
      "|    clip_fraction        | 0.00356       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0397       |\n",
      "|    explained_variance   | 0.495         |\n",
      "|    learning_rate        | 0.000103      |\n",
      "|    loss                 | 1.75          |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | -0.00121      |\n",
      "|    value_loss           | 3.82          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.27          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1781          |\n",
      "|    iterations           | 34            |\n",
      "|    time_elapsed         | 39            |\n",
      "|    total_timesteps      | 69632         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013958433 |\n",
      "|    clip_fraction        | 0.00254       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0358       |\n",
      "|    explained_variance   | 0.502         |\n",
      "|    learning_rate        | 9.72e-05      |\n",
      "|    loss                 | 1.83          |\n",
      "|    n_updates            | 330           |\n",
      "|    policy_gradient_loss | -0.000513     |\n",
      "|    value_loss           | 3.77          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=5.40 +/- 1.50\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 5.4           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 70000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012747478 |\n",
      "|    clip_fraction        | 0.00161       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0333       |\n",
      "|    explained_variance   | 0.5           |\n",
      "|    learning_rate        | 9.11e-05      |\n",
      "|    loss                 | 2.2           |\n",
      "|    n_updates            | 340           |\n",
      "|    policy_gradient_loss | -0.00065      |\n",
      "|    value_loss           | 3.75          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.58     |\n",
      "| time/              |          |\n",
      "|    fps             | 1781     |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.44          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1781          |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 41            |\n",
      "|    total_timesteps      | 73728         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019965085 |\n",
      "|    clip_fraction        | 0.00352       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.031        |\n",
      "|    explained_variance   | 0.508         |\n",
      "|    learning_rate        | 8.5e-05       |\n",
      "|    loss                 | 1.95          |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | -0.00103      |\n",
      "|    value_loss           | 3.7           |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=4.00 +/- 2.10\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 4            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 75000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.594497e-05 |\n",
      "|    clip_fraction        | 0.00137      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0315      |\n",
      "|    explained_variance   | 0.527        |\n",
      "|    learning_rate        | 7.88e-05     |\n",
      "|    loss                 | 2.05         |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.000519    |\n",
      "|    value_loss           | 3.5          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 1781     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 1              |\n",
      "|    ep_rew_mean          | 3.76           |\n",
      "| time/                   |                |\n",
      "|    fps                  | 1780           |\n",
      "|    iterations           | 38             |\n",
      "|    time_elapsed         | 43             |\n",
      "|    total_timesteps      | 77824          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000121895486 |\n",
      "|    clip_fraction        | 0.00151        |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.0296        |\n",
      "|    explained_variance   | 0.488          |\n",
      "|    learning_rate        | 7.27e-05       |\n",
      "|    loss                 | 1.69           |\n",
      "|    n_updates            | 370            |\n",
      "|    policy_gradient_loss | -0.000445      |\n",
      "|    value_loss           | 3.81           |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 3.86         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1781         |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.499054e-05 |\n",
      "|    clip_fraction        | 0.00122      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0281      |\n",
      "|    explained_variance   | 0.514        |\n",
      "|    learning_rate        | 6.65e-05     |\n",
      "|    loss                 | 1.93         |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.000398    |\n",
      "|    value_loss           | 3.75         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=3.60 +/- 3.20\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 3.6           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 80000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.7391876e-05 |\n",
      "|    clip_fraction        | 0.00107       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0257       |\n",
      "|    explained_variance   | 0.535         |\n",
      "|    learning_rate        | 6.04e-05      |\n",
      "|    loss                 | 1.63          |\n",
      "|    n_updates            | 390           |\n",
      "|    policy_gradient_loss | -0.000428     |\n",
      "|    value_loss           | 3.58          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.85     |\n",
      "| time/              |          |\n",
      "|    fps             | 1780     |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.68          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1781          |\n",
      "|    iterations           | 41            |\n",
      "|    time_elapsed         | 47            |\n",
      "|    total_timesteps      | 83968         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.6952733e-05 |\n",
      "|    clip_fraction        | 0.00137       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0244       |\n",
      "|    explained_variance   | 0.489         |\n",
      "|    learning_rate        | 5.42e-05      |\n",
      "|    loss                 | 1.64          |\n",
      "|    n_updates            | 400           |\n",
      "|    policy_gradient_loss | -0.000793     |\n",
      "|    value_loss           | 3.91          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=4.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 4.6          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 85000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.897267e-05 |\n",
      "|    clip_fraction        | 0.00117      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0236      |\n",
      "|    explained_variance   | 0.508        |\n",
      "|    learning_rate        | 4.81e-05     |\n",
      "|    loss                 | 1.86         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.000571    |\n",
      "|    value_loss           | 3.75         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.35     |\n",
      "| time/              |          |\n",
      "|    fps             | 1781     |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 3.29        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1781        |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 3.05851e-05 |\n",
      "|    clip_fraction        | 0.000537    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0232     |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 4.2e-05     |\n",
      "|    loss                 | 1.96        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00039    |\n",
      "|    value_loss           | 3.91        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=3.60 +/- 3.20\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 3.6           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 90000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9983657e-05 |\n",
      "|    clip_fraction        | 0.000342      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.022        |\n",
      "|    explained_variance   | 0.502         |\n",
      "|    learning_rate        | 3.58e-05      |\n",
      "|    loss                 | 2.18          |\n",
      "|    n_updates            | 430           |\n",
      "|    policy_gradient_loss | -0.000343     |\n",
      "|    value_loss           | 3.77          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 4.36     |\n",
      "| time/              |          |\n",
      "|    fps             | 1772     |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.57          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1769          |\n",
      "|    iterations           | 45            |\n",
      "|    time_elapsed         | 52            |\n",
      "|    total_timesteps      | 92160         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014314285 |\n",
      "|    clip_fraction        | 0.00254       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0206       |\n",
      "|    explained_variance   | 0.486         |\n",
      "|    learning_rate        | 2.97e-05      |\n",
      "|    loss                 | 1.79          |\n",
      "|    n_updates            | 440           |\n",
      "|    policy_gradient_loss | -0.00101      |\n",
      "|    value_loss           | 3.84          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.43          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1770          |\n",
      "|    iterations           | 46            |\n",
      "|    time_elapsed         | 53            |\n",
      "|    total_timesteps      | 94208         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.8077596e-05 |\n",
      "|    clip_fraction        | 0.000732      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0208       |\n",
      "|    explained_variance   | 0.494         |\n",
      "|    learning_rate        | 2.35e-05      |\n",
      "|    loss                 | 1.61          |\n",
      "|    n_updates            | 450           |\n",
      "|    policy_gradient_loss | -0.000522     |\n",
      "|    value_loss           | 3.87          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=1.60 +/- 1.96\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 1.6           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 95000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6454782e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0203       |\n",
      "|    explained_variance   | 0.504         |\n",
      "|    learning_rate        | 1.74e-05      |\n",
      "|    loss                 | 1.83          |\n",
      "|    n_updates            | 460           |\n",
      "|    policy_gradient_loss | -0.000127     |\n",
      "|    value_loss           | 3.73          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.2      |\n",
      "| time/              |          |\n",
      "|    fps             | 1770     |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 54       |\n",
      "|    total_timesteps | 96256    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 3.66          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1771          |\n",
      "|    iterations           | 48            |\n",
      "|    time_elapsed         | 55            |\n",
      "|    total_timesteps      | 98304         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0438416e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0205       |\n",
      "|    explained_variance   | 0.49          |\n",
      "|    learning_rate        | 1.12e-05      |\n",
      "|    loss                 | 2.29          |\n",
      "|    n_updates            | 470           |\n",
      "|    policy_gradient_loss | -0.000343     |\n",
      "|    value_loss           | 3.89          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=2.80 +/- 2.79\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | 2.8           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 100000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2205943e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0196       |\n",
      "|    explained_variance   | 0.529         |\n",
      "|    learning_rate        | 5.09e-06      |\n",
      "|    loss                 | 1.84          |\n",
      "|    n_updates            | 480           |\n",
      "|    policy_gradient_loss | -0.000182     |\n",
      "|    value_loss           | 3.52          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 3.53     |\n",
      "| time/              |          |\n",
      "|    fps             | 1771     |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "Total reward after training with PPO: 4\n",
      "Task prize: 5\n",
      "Bidders' bids: [3, 3]\n",
      "Bidders' distances: [2, 3]\n",
      "Distance: 1\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3.common.logger import configure\n",
    "from envs.multiagent_purchasing import MultiAgent_PurchaseEnv\n",
    "import os\n",
    "\n",
    "# Create the environment\n",
    "env = MultiAgent_PurchaseEnv(num_bidders=2)\n",
    "\n",
    "# Configure TensorBoard logging\n",
    "log_dir = \"./runs/baselines/multiagent\"\n",
    "new_logger = configure(log_dir, [\"stdout\", \"tensorboard\"])\n",
    "\n",
    "# Define a learning rate schedule\n",
    "def linear_schedule(initial_value):\n",
    "    def func(progress_remaining):\n",
    "        return progress_remaining * initial_value\n",
    "    return func\n",
    "\n",
    "# Instantiate the agent with adaptive learning rate\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=0, n_steps=2048, batch_size=64, n_epochs=10, learning_rate=linear_schedule(3e-4), tensorboard_log=log_dir)\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# Create callbacks for saving models and evaluation\n",
    "checkpoint_callback = CheckpointCallback(save_freq=int(1e4), save_path=f\"{log_dir}/checkpoints\", name_prefix='ppo_model')\n",
    "eval_callback = EvalCallback(env, best_model_save_path=f\"{log_dir}/best_model\", log_path=log_dir, eval_freq=5000, deterministic=True, render=False)\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(1e5), callback=callback)\n",
    "\n",
    "model_path = os.path.join(log_dir, \"best_model.zip\")\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path)\n",
    "\n",
    "# Test the trained agent\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total reward after training with PPO:\", total_reward)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from envs.multiagent_purchasing import MultiAgent_PurchaseEnv\n",
    "\n",
    "# Update the environment with 20 bidders\n",
    "env = MultiAgent_PurchaseEnv(num_bidders=0)\n",
    "\n",
    "# Run random policy\n",
    "random_rewards = []\n",
    "for _ in range(1000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    random_rewards.append(total_reward)\n",
    "\n",
    "# Run trained policy\n",
    "ppo_rewards = []\n",
    "for _ in range(1000):\n",
    "    obs, _= env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    ppo_rewards.append(total_reward)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(random_rewards, bins=20, alpha=0.5, label='Random Policy', color='#ff7f0e', density=False)\n",
    "plt.hist(ppo_rewards, bins=20, alpha=0.5, label='Trained Policy', color='#1f77b4', density=False)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Agent Reward')\n",
    "plt.ylabel('Frequency (out of 1000)')\n",
    "plt.title('Adversarial Bidding Reward Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task prize: 8\n",
      "Bidders' bids: [6]\n",
      "Bidders' distances: [2]\n",
      "Distance: 3\n",
      "\n",
      "With prize 8 and distance 3, chose action 7\n",
      "Got reward: 0\n"
     ]
    }
   ],
   "source": [
    "# Define a function to run the model and print the chosen action for different numbers of bidders\n",
    "env = MultiAgent_PurchaseEnv(num_bidders=1)\n",
    "obs, _ = env.reset()\n",
    "action, _states = model.predict(obs)\n",
    "\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.render()\n",
    "\n",
    "print(f'\\nWith prize {obs[\"prize\"]} and distance {obs[\"distance\"]}, chose action {action}')\n",
    "\n",
    "print(f\"Got reward: {reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
